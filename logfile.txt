
nhancao@nhancao-windows MINGW64 ~
$ source path.csh
total 529
-rw-r--r-- 1 nhancao 197121   1917 May 30 12:58  MLAgents_EatFruit.sln
drwxr-xr-x 1 nhancao 197121      0 May 30 13:33  venv/
drwxr-xr-x 1 nhancao 197121      0 Jun  3 14:01  obj/
drwxr-xr-x 1 nhancao 197121      0 Jun  6 10:15  MoveAgent2DDemo/
-rw-r--r-- 1 nhancao 197121  43293 Jun  7 23:20 'Cumulative Reward Extend.PNG'
-rw-r--r-- 1 nhancao 197121  49162 Jun  7 23:21  Env.PNG
drwxr-xr-x 1 nhancao 197121      0 Jun 11 14:51  Packages/
-rw-r--r-- 1 nhancao 197121 140309 Jun 11 21:20  logfile.txt
-rwxr-xr-x 1 nhancao 197121    205 Jun 11 21:22  getlog.sh*
-rw-r--r-- 1 nhancao 197121  11854 Jun 11 21:22  filledlog.txt
-rwxr-xr-x 1 nhancao 197121   2908 Jun 12 15:53  incrementalagent.sh*
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:28  Logs/
-rw-r--r-- 1 nhancao 197121  76250 Jun 12 18:29  Assembly-CSharp-firstpass.csproj
-rw-r--r-- 1 nhancao 197121  77483 Jun 12 18:29  Assembly-CSharp.csproj
-rw-r--r-- 1 nhancao 197121  80565 Jun 12 18:29  Assembly-CSharp-Editor.csproj
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:37  UserSettings/
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:38  Build/
-rwxr-xr-x 1 nhancao 197121   3732 Jun 12 22:32  runmlagent.sh*
drwxr-xr-x 1 nhancao 197121      0 Jun 12 22:35  Assets/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 00:12  results/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 00:41  ProjectSettings/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 00:41  Library/
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ ll -rt
total 529
-rw-r--r-- 1 nhancao 197121   1917 May 30 12:58  MLAgents_EatFruit.sln
drwxr-xr-x 1 nhancao 197121      0 May 30 13:33  venv/
drwxr-xr-x 1 nhancao 197121      0 Jun  3 14:01  obj/
drwxr-xr-x 1 nhancao 197121      0 Jun  6 10:15  MoveAgent2DDemo/
-rw-r--r-- 1 nhancao 197121  43293 Jun  7 23:20 'Cumulative Reward Extend.PNG'
-rw-r--r-- 1 nhancao 197121  49162 Jun  7 23:21  Env.PNG
drwxr-xr-x 1 nhancao 197121      0 Jun 11 14:51  Packages/
-rw-r--r-- 1 nhancao 197121 140309 Jun 11 21:20  logfile.txt
-rwxr-xr-x 1 nhancao 197121    205 Jun 11 21:22  getlog.sh*
-rw-r--r-- 1 nhancao 197121  11854 Jun 11 21:22  filledlog.txt
-rwxr-xr-x 1 nhancao 197121   2908 Jun 12 15:53  incrementalagent.sh*
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:28  Logs/
-rw-r--r-- 1 nhancao 197121  76250 Jun 12 18:29  Assembly-CSharp-firstpass.csproj
-rw-r--r-- 1 nhancao 197121  77483 Jun 12 18:29  Assembly-CSharp.csproj
-rw-r--r-- 1 nhancao 197121  80565 Jun 12 18:29  Assembly-CSharp-Editor.csproj
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:37  UserSettings/
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:38  Build/
-rwxr-xr-x 1 nhancao 197121   3732 Jun 12 22:32  runmlagent.sh*
drwxr-xr-x 1 nhancao 197121      0 Jun 12 22:35  Assets/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 00:12  results/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 00:41  ProjectSettings/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 00:41  Library/
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source incrementalagent.sh Player21 3
Initial max_steps: 6300000
  ______   _________     _       _______   _________   _________  _______          _       _____  ____  _____  _____  ____  _____   ______
.' ____ \ |  _   _  |   / \     |_   __ \ |  _   _  | |  _   _  ||_   __ \        / \     |_   _||_   \|_   _||_   _||_   \|_   _|.' ___  |
| (___ \_||_/ | | \_|  / _ \      | |__) ||_/ | | \_| |_/ | | \_|  | |__) |      / _ \      | |    |   \ | |    | |    |   \ | | / .'   \_|
 _.____`.     | |     / ___ \     |  __ /     | |         | |      |  __ /      / ___ \     | |    | |\ \| |    | |    | |\ \| | | |   ____
| \____) |   _| |_  _/ /   \ \_  _| |  \ \_  _| |_       _| |_    _| |  \ \_  _/ /   \ \_  _| |_  _| |_\   |_  _| |_  _| |_\   |_\ `.___]  |
 \______.'  |_____||____| |____||____| |___||_____|     |_____|  |____| |___||____| |____||_____||_____|\____||_____||_____|\____|`._____.'

|---------------------------------------------|
|   Author: PhucThai                          |
|   Date create: June 12 2024                 |
|   Version: 1.0                              |
|   Folder run: results/Player21              |
|   Interation incremental: 3                 |
|   Max_step increase of flow: 7200000 steps |
|---------------------------------------------|


Start interval - max_steps: 6300000 - id=Player21

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
Traceback (most recent call last):
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "D:\UnityProject\MLAgents_EatFruit\venv\Scripts\mlagents-learn.exe\__main__.py", line 7, in <module>
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 264, in main
    run_cli(parse_command_line())
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 260, in run_cli
    run_training(run_seed, options, num_areas)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 75, in run_training
    validate_existing_directories(
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\directory_utils.py", line 25, in validate_existing_directories
    raise UnityTrainerException(
mlagents.trainers.exception.UnityTrainerException: Previous data from this run ID was found. Either specify a new run ID, use --resume to resume this run, or use the --force parameter to overwrite existing data.


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
Sleep a bit!

Interval 1 - max_steps: 6600000 - id=Player21

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      6600000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player21\MoveAgent2D.
[INFO] Resuming training from step 6300411.
[INFO] MoveAgent2D. Step: 6310000. Time Elapsed: 13.425 s. Mean Reward: -1.788. Std of Reward: 0.665. Training.
[INFO] MoveAgent2D. Step: 6320000. Time Elapsed: 20.056 s. Mean Reward: -1.903. Std of Reward: 0.406. Training.
[INFO] MoveAgent2D. Step: 6330000. Time Elapsed: 26.597 s. Mean Reward: -1.888. Std of Reward: 0.488. Training.
[INFO] MoveAgent2D. Step: 6340000. Time Elapsed: 33.958 s. Mean Reward: -1.681. Std of Reward: 0.852. Training.
[INFO] Learning was interrupted. Please wait while the graph is generated.
[INFO] Exported results\Player21\MoveAgent2D\MoveAgent2D-6341497.onnx
Traceback (most recent call last):
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 136, in run_training
    tc.start_learning(env_manager)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents_envs\timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\trainer_controller.py", line 200, in start_learning
    self._save_models()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents_envs\timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\trainer_controller.py", line 80, in _save_models
    self.trainers[brain_name].save_model()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\trainer\rl_trainer.py", line 173, in save_model
    self.model_saver.copy_final_model(model_checkpoint.file_path)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\model_saver\torch_model_saver.py", line 150, in copy_final_model
    shutil.copyfile(source_path, destination_path)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\shutil.py", line 258, in copyfile
    if _HAS_FCOPYFILE:
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "D:\UnityProject\MLAgents_EatFruit\venv\Scripts\mlagents-learn.exe\__main__.py", line 7, in <module>
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 264, in main
    run_cli(parse_command_line())
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 260, in run_cli
    run_training(run_seed, options, num_areas)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 138, in run_training
    env_manager.close()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\subprocess_env_manager.py", line 489, in close
    step: EnvironmentResponse = self.step_queue.get_nowait()
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\queues.py", line 135, in get_nowait
    return self.get(False)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\queues.py", line 115, in get
    elif not self._poll():
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\connection.py", line 262, in poll
    return self._poll(timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\connection.py", line 335, in _poll
    return bool(wait([self], timeout))
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\connection.py", line 888, in wait
    ov.cancel()
KeyboardInterrupt
Process Process-1:
Traceback (most recent call last):
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 315, in _bootstrap
    self.run()
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\subprocess_env_manager.py", line 239, in worker
    env.close()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents_envs\environment.py", line 425, in close
    self._close()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents_envs\environment.py", line 443, in _close
    self._process.wait(timeout=timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\subprocess.py", line 1204, in wait
    return self._wait(timeout=timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\subprocess.py", line 1485, in _wait
    result = _winapi.WaitForSingleObject(self._handle,
KeyboardInterrupt


(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ vim incrementalagent.sh
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source runmlagent.sh Player22 20
  ______   _________     _       _______   _________   _________  _______          _       _____  ____  _____  _____  ____  _____   ______
.' ____ \ |  _   _  |   / \     |_   __ \ |  _   _  | |  _   _  ||_   __ \        / \     |_   _||_   \|_   _||_   _||_   \|_   _|.' ___  |
| (___ \_||_/ | | \_|  / _ \      | |__) ||_/ | | \_| |_/ | | \_|  | |__) |      / _ \      | |    |   \ | |    | |    |   \ | | / .'   \_|
 _.____`.     | |     / ___ \     |  __ /     | |         | |      |  __ /      / ___ \     | |    | |\ \| |    | |    | |\ \| | | |   ____
| \____) |   _| |_  _/ /   \ \_  _| |  \ \_  _| |_       _| |_    _| |  \ \_  _/ /   \ \_  _| |_  _| |_\   |_  _| |_  _| |_\   |_\ `.___]  |
 \______.'  |_____||____| |____||____| |___||_____|     |_____|  |____| |___||____| |____||_____||_____|\____||_____||_____|\____|`._____.'

|-------------------------------------|
|   Author: PhucThai                  |
|   Date create: June 11 2024         |
|   Version: 1.0                      |
|   Folder run: results/Player22      |
|   Interation: 20                    |
|   Max_step of flow: 6000000 steps   |
|-------------------------------------|


Start interval - max_steps: 300000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.94
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   0.7
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
          gail:
            gamma:      0.99
            strength:   0.1
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
            use_actions:        False
            use_vail:   False
            demo_path:  MoveAgent2DDemo/MoveAgent2D.demo
          curiosity:
            gamma:      0.99
            strength:   0.01
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      300000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:
          demo_path:    MoveAgent2DDemo/MoveAgent2D.demo
          steps:        0
          strength:     0.3
          samples_per_update:   0
          num_epoch:    None
          batch_size:   None
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\torch_entities\utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3641.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] MoveAgent2D. Step: 10000. Time Elapsed: 10.521 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 20000. Time Elapsed: 17.674 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 30000. Time Elapsed: 24.320 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 40000. Time Elapsed: 32.037 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] Learning was interrupted. Please wait while the graph is generated.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-41088.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-41088.onnx to results\Player22\MoveAgent2D.onnx.
Traceback (most recent call last):
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "D:\UnityProject\MLAgents_EatFruit\venv\Scripts\mlagents-learn.exe\__main__.py", line 7, in <module>
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 264, in main
    run_cli(parse_command_line())
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 260, in run_cli
    run_training(run_seed, options, num_areas)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\learn.py", line 138, in run_training
    env_manager.close()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\subprocess_env_manager.py", line 489, in close
    step: EnvironmentResponse = self.step_queue.get_nowait()
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\queues.py", line 135, in get_nowait
    return self.get(False)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\queues.py", line 115, in get
    elif not self._poll():
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\connection.py", line 262, in poll
    return self._poll(timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\connection.py", line 335, in _poll
    return bool(wait([self], timeout))
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\connection.py", line 888, in wait
    ov.cancel()
KeyboardInterrupt
Process Process-1:
Traceback (most recent call last):
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 315, in _bootstrap
    self.run()
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\subprocess_env_manager.py", line 239, in worker
    env.close()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents_envs\environment.py", line 425, in close
    self._close()
  File "D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents_envs\environment.py", line 443, in _close
    self._process.wait(timeout=timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\subprocess.py", line 1204, in wait
    return self._wait(timeout=timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\subprocess.py", line 1485, in _wait
    result = _winapi.WaitForSingleObject(self._handle,
KeyboardInterrupt
Exception ignored in atexit callback: <function _exit_function at 0x0000024272277640>
Traceback (most recent call last):
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\util.py", line 357, in _exit_function
    p.join()
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "C:\Users\nhancao\AppData\Local\Programs\Python\Python310\lib\multiprocessing\popen_spawn_win32.py", line 108, in wait
    res = _winapi.WaitForSingleObject(int(self._handle), msecs)
KeyboardInterrupt:

(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source runmlagent.sh Player22 20
  ______   _________     _       _______   _________   _________  _______          _       _____  ____  _____  _____  ____  _____   ______
.' ____ \ |  _   _  |   / \     |_   __ \ |  _   _  | |  _   _  ||_   __ \        / \     |_   _||_   \|_   _||_   _||_   \|_   _|.' ___  |
| (___ \_||_/ | | \_|  / _ \      | |__) ||_/ | | \_| |_/ | | \_|  | |__) |      / _ \      | |    |   \ | |    | |    |   \ | | / .'   \_|
 _.____`.     | |     / ___ \     |  __ /     | |         | |      |  __ /      / ___ \     | |    | |\ \| |    | |    | |\ \| | | |   ____
| \____) |   _| |_  _/ /   \ \_  _| |  \ \_  _| |_       _| |_    _| |  \ \_  _/ /   \ \_  _| |_  _| |_\   |_  _| |_  _| |_\   |_\ `.___]  |
 \______.'  |_____||____| |____||____| |___||_____|     |_____|  |____| |___||____| |____||_____||_____|\____||_____||_____|\____|`._____.'

|-------------------------------------|
|   Author: PhucThai                  |
|   Date create: June 11 2024         |
|   Version: 1.0                      |
|   Folder run: results/Player22      |
|   Interation: 20                    |
|   Max_step of flow: 6000000 steps   |
|-------------------------------------|


Start interval - max_steps: 300000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.94
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   0.7
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
          gail:
            gamma:      0.99
            strength:   0.1
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
            use_actions:        False
            use_vail:   False
            demo_path:  MoveAgent2DDemo/MoveAgent2D.demo
          curiosity:
            gamma:      0.99
            strength:   0.01
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      300000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:
          demo_path:    MoveAgent2DDemo/MoveAgent2D.demo
          steps:        0
          strength:     0.3
          samples_per_update:   0
          num_epoch:    None
          batch_size:   None
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\torch_entities\utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3641.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] MoveAgent2D. Step: 10000. Time Elapsed: 14.558 s. Mean Reward: -1.767. Std of Reward: 0.750. Training.
[INFO] MoveAgent2D. Step: 20000. Time Elapsed: 22.481 s. Mean Reward: -1.875. Std of Reward: 0.552. Training.
[INFO] MoveAgent2D. Step: 30000. Time Elapsed: 31.089 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 40000. Time Elapsed: 39.971 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 50000. Time Elapsed: 72.292 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 60000. Time Elapsed: 80.668 s. Mean Reward: -1.872. Std of Reward: 0.567. Training.
[INFO] MoveAgent2D. Step: 70000. Time Elapsed: 89.547 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 80000. Time Elapsed: 97.932 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 90000. Time Elapsed: 128.377 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 100000. Time Elapsed: 136.633 s. Mean Reward: -1.896. Std of Reward: 0.486. Training.
[INFO] MoveAgent2D. Step: 110000. Time Elapsed: 143.973 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 120000. Time Elapsed: 151.296 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 130000. Time Elapsed: 182.283 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 140000. Time Elapsed: 190.059 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 150000. Time Elapsed: 197.366 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 160000. Time Elapsed: 205.400 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 170000. Time Elapsed: 236.478 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 180000. Time Elapsed: 243.917 s. Mean Reward: -1.792. Std of Reward: 0.696. Training.
[INFO] MoveAgent2D. Step: 190000. Time Elapsed: 251.495 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 200000. Time Elapsed: 259.527 s. Mean Reward: -1.809. Std of Reward: 0.647. Training.
[INFO] MoveAgent2D. Step: 210000. Time Elapsed: 290.206 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 220000. Time Elapsed: 297.899 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 230000. Time Elapsed: 306.002 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 240000. Time Elapsed: 313.313 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 250000. Time Elapsed: 344.229 s. Mean Reward: -1.903. Std of Reward: 0.413. Training.
[INFO] MoveAgent2D. Step: 260000. Time Elapsed: 352.314 s. Mean Reward: -1.675. Std of Reward: 0.885. Training.
[INFO] MoveAgent2D. Step: 270000. Time Elapsed: 359.759 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 280000. Time Elapsed: 367.234 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 290000. Time Elapsed: 398.440 s. Mean Reward: -1.779. Std of Reward: 0.753. Training.
[INFO] MoveAgent2D. Step: 300000. Time Elapsed: 406.248 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-300377.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-300377.onnx to results\Player22\MoveAgent2D.onnx.
Sleep a bit!

Interval 1 - max_steps: 600000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.94
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   0.7
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
          gail:
            gamma:      0.99
            strength:   0.1
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
            use_actions:        False
            use_vail:   False
            demo_path:  MoveAgent2DDemo/MoveAgent2D.demo
          curiosity:
            gamma:      0.99
            strength:   0.01
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      600000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:
          demo_path:    MoveAgent2DDemo/MoveAgent2D.demo
          steps:        0
          strength:     0.3
          samples_per_update:   0
          num_epoch:    None
          batch_size:   None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 300377.
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\torch_entities\utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3641.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] MoveAgent2D. Step: 310000. Time Elapsed: 13.428 s. Mean Reward: -1.326. Std of Reward: 1.152. Training.
[INFO] MoveAgent2D. Step: 320000. Time Elapsed: 21.627 s. Mean Reward: -1.699. Std of Reward: 0.794. Training.
[INFO] MoveAgent2D. Step: 330000. Time Elapsed: 30.138 s. Mean Reward: -1.357. Std of Reward: 1.103. Training.
[INFO] MoveAgent2D. Step: 340000. Time Elapsed: 38.777 s. Mean Reward: -1.814. Std of Reward: 0.590. Training.
[INFO] MoveAgent2D. Step: 350000. Time Elapsed: 71.928 s. Mean Reward: -1.512. Std of Reward: 1.009. Training.
[INFO] MoveAgent2D. Step: 360000. Time Elapsed: 80.253 s. Mean Reward: -1.795. Std of Reward: 0.659. Training.
[INFO] MoveAgent2D. Step: 370000. Time Elapsed: 89.043 s. Mean Reward: -1.887. Std of Reward: 0.514. Training.
[INFO] MoveAgent2D. Step: 380000. Time Elapsed: 97.957 s. Mean Reward: -1.881. Std of Reward: 0.533. Training.
[INFO] MoveAgent2D. Step: 390000. Time Elapsed: 130.950 s. Mean Reward: -1.906. Std of Reward: 0.421. Training.
[INFO] MoveAgent2D. Step: 400000. Time Elapsed: 139.687 s. Mean Reward: -1.887. Std of Reward: 0.494. Training.
[INFO] MoveAgent2D. Step: 410000. Time Elapsed: 148.102 s. Mean Reward: -1.907. Std of Reward: 0.405. Training.
[INFO] MoveAgent2D. Step: 420000. Time Elapsed: 156.788 s. Mean Reward: -1.800. Std of Reward: 0.657. Training.
[INFO] MoveAgent2D. Step: 430000. Time Elapsed: 189.782 s. Mean Reward: -1.783. Std of Reward: 0.712. Training.
[INFO] MoveAgent2D. Step: 440000. Time Elapsed: 198.286 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 450000. Time Elapsed: 206.896 s. Mean Reward: -1.810. Std of Reward: 0.601. Training.
[INFO] MoveAgent2D. Step: 460000. Time Elapsed: 215.711 s. Mean Reward: -1.637. Std of Reward: 0.867. Training.
[INFO] MoveAgent2D. Step: 470000. Time Elapsed: 248.489 s. Mean Reward: -1.903. Std of Reward: 0.413. Training.
[INFO] MoveAgent2D. Step: 480000. Time Elapsed: 257.069 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 490000. Time Elapsed: 265.458 s. Mean Reward: -1.908. Std of Reward: 0.405. Training.
[INFO] MoveAgent2D. Step: 500000. Time Elapsed: 274.245 s. Mean Reward: -1.887. Std of Reward: 0.504. Training.
[INFO] MoveAgent2D. Step: 510000. Time Elapsed: 307.608 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 520000. Time Elapsed: 315.952 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 530000. Time Elapsed: 324.641 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 540000. Time Elapsed: 333.166 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 550000. Time Elapsed: 366.327 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 560000. Time Elapsed: 374.869 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 570000. Time Elapsed: 383.395 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 580000. Time Elapsed: 391.725 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 590000. Time Elapsed: 424.938 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 600000. Time Elapsed: 433.413 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-600092.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-600092.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 2 - max_steps: 900000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9425
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   0.7
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
          gail:
            gamma:      0.99
            strength:   0.1
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
            use_actions:        False
            use_vail:   False
            demo_path:  MoveAgent2DDemo/MoveAgent2D.demo
          curiosity:
            gamma:      0.99
            strength:   0.01
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      900000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:
          demo_path:    MoveAgent2DDemo/MoveAgent2D.demo
          steps:        0
          strength:     0.3
          samples_per_update:   0
          num_epoch:    None
          batch_size:   None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 600092.
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\torch_entities\utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3641.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] MoveAgent2D. Step: 610000. Time Elapsed: 13.781 s. Mean Reward: -1.403. Std of Reward: 1.074. Training.
[INFO] MoveAgent2D. Step: 620000. Time Elapsed: 21.709 s. Mean Reward: -1.275. Std of Reward: 1.164. Training.
[INFO] MoveAgent2D. Step: 630000. Time Elapsed: 30.111 s. Mean Reward: -1.437. Std of Reward: 1.034. Training.
[INFO] MoveAgent2D. Step: 640000. Time Elapsed: 39.000 s. Mean Reward: -1.700. Std of Reward: 0.818. Training.
[INFO] MoveAgent2D. Step: 650000. Time Elapsed: 71.925 s. Mean Reward: -1.798. Std of Reward: 0.661. Training.
[INFO] MoveAgent2D. Step: 660000. Time Elapsed: 80.262 s. Mean Reward: -1.532. Std of Reward: 1.000. Training.
[INFO] MoveAgent2D. Step: 670000. Time Elapsed: 89.219 s. Mean Reward: -1.303. Std of Reward: 1.166. Training.
[INFO] MoveAgent2D. Step: 680000. Time Elapsed: 97.893 s. Mean Reward: -1.772. Std of Reward: 0.748. Training.
[INFO] MoveAgent2D. Step: 690000. Time Elapsed: 130.800 s. Mean Reward: -1.899. Std of Reward: 0.434. Training.
[INFO] MoveAgent2D. Step: 700000. Time Elapsed: 139.472 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 710000. Time Elapsed: 148.247 s. Mean Reward: -1.814. Std of Reward: 0.616. Training.
[INFO] MoveAgent2D. Step: 720000. Time Elapsed: 156.924 s. Mean Reward: -1.794. Std of Reward: 0.691. Training.
[INFO] MoveAgent2D. Step: 730000. Time Elapsed: 189.666 s. Mean Reward: -1.887. Std of Reward: 0.481. Training.
[INFO] MoveAgent2D. Step: 740000. Time Elapsed: 198.487 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 750000. Time Elapsed: 206.963 s. Mean Reward: -1.883. Std of Reward: 0.525. Training.
[INFO] MoveAgent2D. Step: 760000. Time Elapsed: 215.417 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 770000. Time Elapsed: 248.586 s. Mean Reward: -1.874. Std of Reward: 0.566. Training.
[INFO] MoveAgent2D. Step: 780000. Time Elapsed: 257.131 s. Mean Reward: -1.881. Std of Reward: 0.532. Training.
[INFO] MoveAgent2D. Step: 790000. Time Elapsed: 265.666 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 800000. Time Elapsed: 274.463 s. Mean Reward: -1.883. Std of Reward: 0.525. Training.
[INFO] MoveAgent2D. Step: 810000. Time Elapsed: 307.217 s. Mean Reward: -1.815. Std of Reward: 0.615. Training.
[INFO] MoveAgent2D. Step: 820000. Time Elapsed: 315.625 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 830000. Time Elapsed: 324.439 s. Mean Reward: -1.806. Std of Reward: 0.649. Training.
[INFO] MoveAgent2D. Step: 840000. Time Elapsed: 332.977 s. Mean Reward: -1.896. Std of Reward: 0.440. Training.
[INFO] MoveAgent2D. Step: 850000. Time Elapsed: 365.680 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 860000. Time Elapsed: 374.170 s. Mean Reward: -1.895. Std of Reward: 0.464. Training.
[INFO] MoveAgent2D. Step: 870000. Time Elapsed: 382.964 s. Mean Reward: -1.902. Std of Reward: 0.420. Training.
[INFO] MoveAgent2D. Step: 880000. Time Elapsed: 391.272 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 890000. Time Elapsed: 424.277 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 900000. Time Elapsed: 432.966 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-900011.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-900011.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 3 - max_steps: 1200000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.945
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   0.7
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
          gail:
            gamma:      0.99
            strength:   0.1
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
            use_actions:        False
            use_vail:   False
            demo_path:  MoveAgent2DDemo/MoveAgent2D.demo
          curiosity:
            gamma:      0.99
            strength:   0.01
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
            learning_rate:      0.0003
            encoding_size:      None
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      1200000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:
          demo_path:    MoveAgent2DDemo/MoveAgent2D.demo
          steps:        0
          strength:     0.3
          samples_per_update:   0
          num_epoch:    None
          batch_size:   None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 900011.
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\mlagents\trainers\torch_entities\utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3641.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] MoveAgent2D. Step: 910000. Time Elapsed: 13.716 s. Mean Reward: -1.510. Std of Reward: 0.986. Training.
[INFO] MoveAgent2D. Step: 920000. Time Elapsed: 21.664 s. Mean Reward: -1.794. Std of Reward: 0.668. Training.
[INFO] MoveAgent2D. Step: 930000. Time Elapsed: 30.207 s. Mean Reward: -1.699. Std of Reward: 0.804. Training.
[INFO] MoveAgent2D. Step: 940000. Time Elapsed: 38.650 s. Mean Reward: -1.793. Std of Reward: 0.682. Training.
[INFO] MoveAgent2D. Step: 950000. Time Elapsed: 71.665 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 960000. Time Elapsed: 80.239 s. Mean Reward: -1.891. Std of Reward: 0.473. Training.
[INFO] MoveAgent2D. Step: 970000. Time Elapsed: 88.931 s. Mean Reward: -1.890. Std of Reward: 0.499. Training.
[INFO] MoveAgent2D. Step: 980000. Time Elapsed: 97.330 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 990000. Time Elapsed: 130.653 s. Mean Reward: -1.721. Std of Reward: 0.813. Training.
[INFO] MoveAgent2D. Step: 1000000. Time Elapsed: 139.269 s. Mean Reward: -1.900. Std of Reward: 0.438. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-999981.onnx
[INFO] MoveAgent2D. Step: 1010000. Time Elapsed: 148.040 s. Mean Reward: -1.892. Std of Reward: 0.458. Training.
[INFO] MoveAgent2D. Step: 1020000. Time Elapsed: 156.654 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1030000. Time Elapsed: 189.424 s. Mean Reward: -1.893. Std of Reward: 0.462. Training.
[INFO] MoveAgent2D. Step: 1040000. Time Elapsed: 198.050 s. Mean Reward: -1.899. Std of Reward: 0.433. Training.
[INFO] MoveAgent2D. Step: 1050000. Time Elapsed: 206.625 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1060000. Time Elapsed: 215.029 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1070000. Time Elapsed: 248.160 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1080000. Time Elapsed: 256.611 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1090000. Time Elapsed: 265.121 s. Mean Reward: -1.899. Std of Reward: 0.443. Training.
[INFO] MoveAgent2D. Step: 1100000. Time Elapsed: 273.731 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1110000. Time Elapsed: 306.707 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1120000. Time Elapsed: 315.102 s. Mean Reward: -1.888. Std of Reward: 0.489. Training.
[INFO] MoveAgent2D. Step: 1130000. Time Elapsed: 323.338 s. Mean Reward: -1.793. Std of Reward: 0.673. Training.
[INFO] MoveAgent2D. Step: 1140000. Time Elapsed: 332.207 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1150000. Time Elapsed: 364.979 s. Mean Reward: -1.772. Std of Reward: 0.731. Training.
[INFO] MoveAgent2D. Step: 1160000. Time Elapsed: 372.969 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1170000. Time Elapsed: 381.555 s. Mean Reward: -1.907. Std of Reward: 0.403. Training.
[INFO] MoveAgent2D. Step: 1180000. Time Elapsed: 390.007 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1190000. Time Elapsed: 422.682 s. Mean Reward: -1.884. Std of Reward: 0.528. Training.
[INFO] MoveAgent2D. Step: 1200000. Time Elapsed: 430.964 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-1200187.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-1200187.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 4 - max_steps: 1500000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9475
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      1500000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[WARNING] Failed to load for module Optimizer:value_optimizer. Initializing
[WARNING] Did not expect these keys ['value_heads.value_heads.gail.weight', 'value_heads.value_heads.gail.bias', 'value_heads.value_heads.curiosity.weight', 'value_heads.value_heads.curiosity.bias'] in checkpoint. Ignoring.
[INFO] Resuming training from step 1200187.
[INFO] MoveAgent2D. Step: 1210000. Time Elapsed: 11.770 s. Mean Reward: -1.880. Std of Reward: 0.526. Training.
[INFO] MoveAgent2D. Step: 1220000. Time Elapsed: 18.630 s. Mean Reward: -1.884. Std of Reward: 0.494. Training.
[INFO] MoveAgent2D. Step: 1230000. Time Elapsed: 26.207 s. Mean Reward: -1.788. Std of Reward: 0.683. Training.
[INFO] MoveAgent2D. Step: 1240000. Time Elapsed: 34.728 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1250000. Time Elapsed: 52.440 s. Mean Reward: -1.605. Std of Reward: 0.906. Training.
[INFO] MoveAgent2D. Step: 1260000. Time Elapsed: 60.461 s. Mean Reward: -1.889. Std of Reward: 0.481. Training.
[INFO] MoveAgent2D. Step: 1270000. Time Elapsed: 68.343 s. Mean Reward: -1.547. Std of Reward: 0.989. Training.
[INFO] MoveAgent2D. Step: 1280000. Time Elapsed: 76.471 s. Mean Reward: -1.790. Std of Reward: 0.686. Training.
[INFO] MoveAgent2D. Step: 1290000. Time Elapsed: 94.520 s. Mean Reward: -1.819. Std of Reward: 0.611. Training.
[INFO] MoveAgent2D. Step: 1300000. Time Elapsed: 102.625 s. Mean Reward: -1.334. Std of Reward: 1.096. Training.
[INFO] MoveAgent2D. Step: 1310000. Time Elapsed: 110.526 s. Mean Reward: -1.494. Std of Reward: 1.065. Training.
[INFO] MoveAgent2D. Step: 1320000. Time Elapsed: 118.838 s. Mean Reward: -1.902. Std of Reward: 0.410. Training.
[INFO] MoveAgent2D. Step: 1330000. Time Elapsed: 136.817 s. Mean Reward: -1.683. Std of Reward: 0.831. Training.
[INFO] MoveAgent2D. Step: 1340000. Time Elapsed: 144.766 s. Mean Reward: -1.470. Std of Reward: 0.999. Training.
[INFO] MoveAgent2D. Step: 1350000. Time Elapsed: 153.025 s. Mean Reward: -1.404. Std of Reward: 1.090. Training.
[INFO] MoveAgent2D. Step: 1360000. Time Elapsed: 161.284 s. Mean Reward: -1.395. Std of Reward: 1.062. Training.
[INFO] MoveAgent2D. Step: 1370000. Time Elapsed: 179.459 s. Mean Reward: -1.611. Std of Reward: 0.929. Training.
[INFO] MoveAgent2D. Step: 1380000. Time Elapsed: 187.781 s. Mean Reward: -1.643. Std of Reward: 0.827. Training.
[INFO] MoveAgent2D. Step: 1390000. Time Elapsed: 196.138 s. Mean Reward: -1.617. Std of Reward: 0.909. Training.
[INFO] MoveAgent2D. Step: 1400000. Time Elapsed: 204.253 s. Mean Reward: -1.367. Std of Reward: 1.023. Training.
[INFO] MoveAgent2D. Step: 1410000. Time Elapsed: 222.203 s. Mean Reward: -1.194. Std of Reward: 1.199. Training.
[INFO] MoveAgent2D. Step: 1420000. Time Elapsed: 230.511 s. Mean Reward: -1.443. Std of Reward: 1.070. Training.
[INFO] MoveAgent2D. Step: 1430000. Time Elapsed: 238.711 s. Mean Reward: -1.788. Std of Reward: 0.718. Training.
[INFO] MoveAgent2D. Step: 1440000. Time Elapsed: 246.746 s. Mean Reward: -1.800. Std of Reward: 0.650. Training.
[INFO] MoveAgent2D. Step: 1450000. Time Elapsed: 265.433 s. Mean Reward: -1.735. Std of Reward: 0.700. Training.
[INFO] MoveAgent2D. Step: 1460000. Time Elapsed: 273.558 s. Mean Reward: -1.305. Std of Reward: 1.184. Training.
[INFO] MoveAgent2D. Step: 1470000. Time Elapsed: 281.754 s. Mean Reward: -1.818. Std of Reward: 0.613. Training.
[INFO] MoveAgent2D. Step: 1480000. Time Elapsed: 289.932 s. Mean Reward: -1.798. Std of Reward: 0.689. Training.
[INFO] MoveAgent2D. Step: 1490000. Time Elapsed: 308.429 s. Mean Reward: -1.693. Std of Reward: 0.852. Training.
[INFO] MoveAgent2D. Step: 1500000. Time Elapsed: 316.495 s. Mean Reward: -1.387. Std of Reward: 1.100. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-1500023.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-1500023.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 5 - max_steps: 1800000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.95
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      1800000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 1500023.
[INFO] MoveAgent2D. Step: 1510000. Time Elapsed: 12.201 s. Mean Reward: -1.113. Std of Reward: 1.252. Training.
[INFO] MoveAgent2D. Step: 1520000. Time Elapsed: 19.655 s. Mean Reward: -0.886. Std of Reward: 1.236. Training.
[INFO] MoveAgent2D. Step: 1530000. Time Elapsed: 27.411 s. Mean Reward: -1.291. Std of Reward: 1.184. Training.
[INFO] MoveAgent2D. Step: 1540000. Time Elapsed: 35.131 s. Mean Reward: -1.500. Std of Reward: 1.003. Training.
[INFO] MoveAgent2D. Step: 1550000. Time Elapsed: 53.044 s. Mean Reward: -1.381. Std of Reward: 1.085. Training.
[INFO] MoveAgent2D. Step: 1560000. Time Elapsed: 60.351 s. Mean Reward: -1.790. Std of Reward: 0.712. Training.
[INFO] MoveAgent2D. Step: 1570000. Time Elapsed: 67.878 s. Mean Reward: -1.542. Std of Reward: 0.963. Training.
[INFO] MoveAgent2D. Step: 1580000. Time Elapsed: 75.324 s. Mean Reward: -1.215. Std of Reward: 1.192. Training.
[INFO] MoveAgent2D. Step: 1590000. Time Elapsed: 93.387 s. Mean Reward: -1.092. Std of Reward: 1.222. Training.
[INFO] MoveAgent2D. Step: 1600000. Time Elapsed: 100.710 s. Mean Reward: -1.498. Std of Reward: 1.032. Training.
[INFO] MoveAgent2D. Step: 1610000. Time Elapsed: 108.422 s. Mean Reward: -1.370. Std of Reward: 1.107. Training.
[INFO] MoveAgent2D. Step: 1620000. Time Elapsed: 115.841 s. Mean Reward: -1.274. Std of Reward: 1.167. Training.
[INFO] MoveAgent2D. Step: 1630000. Time Elapsed: 133.338 s. Mean Reward: -1.270. Std of Reward: 1.173. Training.
[INFO] MoveAgent2D. Step: 1640000. Time Elapsed: 140.972 s. Mean Reward: -1.423. Std of Reward: 1.095. Training.
[INFO] MoveAgent2D. Step: 1650000. Time Elapsed: 148.136 s. Mean Reward: -1.018. Std of Reward: 1.283. Training.
[INFO] MoveAgent2D. Step: 1660000. Time Elapsed: 155.750 s. Mean Reward: -1.168. Std of Reward: 1.240. Training.
[INFO] MoveAgent2D. Step: 1670000. Time Elapsed: 173.446 s. Mean Reward: -1.368. Std of Reward: 1.080. Training.
[INFO] MoveAgent2D. Step: 1680000. Time Elapsed: 181.019 s. Mean Reward: -0.693. Std of Reward: 1.309. Training.
[INFO] MoveAgent2D. Step: 1690000. Time Elapsed: 188.460 s. Mean Reward: -0.930. Std of Reward: 1.304. Training.
[INFO] MoveAgent2D. Step: 1700000. Time Elapsed: 195.954 s. Mean Reward: -0.956. Std of Reward: 1.290. Training.
[INFO] MoveAgent2D. Step: 1710000. Time Elapsed: 213.891 s. Mean Reward: -1.302. Std of Reward: 1.190. Training.
[INFO] MoveAgent2D. Step: 1720000. Time Elapsed: 221.209 s. Mean Reward: -1.988. Std of Reward: 0.000. Training.
[INFO] MoveAgent2D. Step: 1730000. Time Elapsed: 228.833 s. Mean Reward: -1.638. Std of Reward: 0.844. Training.
[INFO] MoveAgent2D. Step: 1740000. Time Elapsed: 236.381 s. Mean Reward: -1.684. Std of Reward: 0.860. Training.
[INFO] MoveAgent2D. Step: 1750000. Time Elapsed: 253.906 s. Mean Reward: -1.444. Std of Reward: 1.074. Training.
[INFO] MoveAgent2D. Step: 1760000. Time Elapsed: 261.556 s. Mean Reward: -0.848. Std of Reward: 1.318. Training.
[INFO] MoveAgent2D. Step: 1770000. Time Elapsed: 269.046 s. Mean Reward: -1.885. Std of Reward: 0.500. Training.
[INFO] MoveAgent2D. Step: 1780000. Time Elapsed: 276.377 s. Mean Reward: -1.090. Std of Reward: 1.248. Training.
[INFO] MoveAgent2D. Step: 1790000. Time Elapsed: 294.443 s. Mean Reward: -1.716. Std of Reward: 0.772. Training.
[INFO] MoveAgent2D. Step: 1800000. Time Elapsed: 301.964 s. Mean Reward: -0.910. Std of Reward: 1.333. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-1800177.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-1800177.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 6 - max_steps: 2100000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9525
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      2100000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 1800177.
[INFO] MoveAgent2D. Step: 1810000. Time Elapsed: 12.286 s. Mean Reward: -0.401. Std of Reward: 1.337. Training.
[INFO] MoveAgent2D. Step: 1820000. Time Elapsed: 19.706 s. Mean Reward: -0.831. Std of Reward: 1.361. Training.
[INFO] MoveAgent2D. Step: 1830000. Time Elapsed: 28.379 s. Mean Reward: -0.861. Std of Reward: 1.282. Training.
[INFO] MoveAgent2D. Step: 1840000. Time Elapsed: 36.692 s. Mean Reward: -1.216. Std of Reward: 1.188. Training.
[INFO] MoveAgent2D. Step: 1850000. Time Elapsed: 54.909 s. Mean Reward: -1.032. Std of Reward: 1.255. Training.
[INFO] MoveAgent2D. Step: 1860000. Time Elapsed: 63.122 s. Mean Reward: -1.205. Std of Reward: 1.180. Training.
[INFO] MoveAgent2D. Step: 1870000. Time Elapsed: 71.574 s. Mean Reward: -1.139. Std of Reward: 1.188. Training.
[INFO] MoveAgent2D. Step: 1880000. Time Elapsed: 79.904 s. Mean Reward: -0.552. Std of Reward: 1.339. Training.
[INFO] MoveAgent2D. Step: 1890000. Time Elapsed: 98.483 s. Mean Reward: -0.670. Std of Reward: 1.361. Training.
[INFO] MoveAgent2D. Step: 1900000. Time Elapsed: 106.680 s. Mean Reward: -0.547. Std of Reward: 1.306. Training.
[INFO] MoveAgent2D. Step: 1910000. Time Elapsed: 115.005 s. Mean Reward: -0.789. Std of Reward: 1.319. Training.
[INFO] MoveAgent2D. Step: 1920000. Time Elapsed: 123.151 s. Mean Reward: -0.948. Std of Reward: 1.329. Training.
[INFO] MoveAgent2D. Step: 1930000. Time Elapsed: 141.816 s. Mean Reward: -0.988. Std of Reward: 1.337. Training.
[INFO] MoveAgent2D. Step: 1940000. Time Elapsed: 150.072 s. Mean Reward: -0.471. Std of Reward: 1.352. Training.
[INFO] MoveAgent2D. Step: 1950000. Time Elapsed: 158.404 s. Mean Reward: -0.626. Std of Reward: 1.368. Training.
[INFO] MoveAgent2D. Step: 1960000. Time Elapsed: 166.921 s. Mean Reward: -0.592. Std of Reward: 1.303. Training.
[INFO] MoveAgent2D. Step: 1970000. Time Elapsed: 185.351 s. Mean Reward: -0.983. Std of Reward: 1.321. Training.
[INFO] MoveAgent2D. Step: 1980000. Time Elapsed: 193.573 s. Mean Reward: -0.609. Std of Reward: 1.384. Training.
[INFO] MoveAgent2D. Step: 1990000. Time Elapsed: 201.956 s. Mean Reward: -1.037. Std of Reward: 1.316. Training.
[INFO] MoveAgent2D. Step: 2000000. Time Elapsed: 210.188 s. Mean Reward: -0.264. Std of Reward: 1.280. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-1999940.onnx
[INFO] MoveAgent2D. Step: 2010000. Time Elapsed: 228.543 s. Mean Reward: -0.549. Std of Reward: 1.329. Training.
[INFO] MoveAgent2D. Step: 2020000. Time Elapsed: 236.950 s. Mean Reward: -0.434. Std of Reward: 1.351. Training.
[INFO] MoveAgent2D. Step: 2030000. Time Elapsed: 245.241 s. Mean Reward: -0.961. Std of Reward: 1.281. Training.
[INFO] MoveAgent2D. Step: 2040000. Time Elapsed: 253.633 s. Mean Reward: -0.207. Std of Reward: 1.312. Training.
[INFO] MoveAgent2D. Step: 2050000. Time Elapsed: 272.469 s. Mean Reward: -0.440. Std of Reward: 1.375. Training.
[INFO] MoveAgent2D. Step: 2060000. Time Elapsed: 280.780 s. Mean Reward: -0.760. Std of Reward: 1.343. Training.
[INFO] MoveAgent2D. Step: 2070000. Time Elapsed: 289.111 s. Mean Reward: -0.413. Std of Reward: 1.352. Training.
[INFO] MoveAgent2D. Step: 2080000. Time Elapsed: 297.565 s. Mean Reward: -0.298. Std of Reward: 1.364. Training.
[INFO] MoveAgent2D. Step: 2090000. Time Elapsed: 316.019 s. Mean Reward: -0.629. Std of Reward: 1.370. Training.
[INFO] MoveAgent2D. Step: 2100000. Time Elapsed: 324.017 s. Mean Reward: -1.065. Std of Reward: 1.208. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-2100060.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-2100060.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 7 - max_steps: 2400000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.955
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      2400000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 2100060.
[INFO] MoveAgent2D. Step: 2110000. Time Elapsed: 12.183 s. Mean Reward: 0.348. Std of Reward: 0.977. Training.
[INFO] MoveAgent2D. Step: 2120000. Time Elapsed: 20.061 s. Mean Reward: -0.398. Std of Reward: 1.316. Training.
[INFO] MoveAgent2D. Step: 2130000. Time Elapsed: 28.553 s. Mean Reward: -0.196. Std of Reward: 1.222. Training.
[INFO] MoveAgent2D. Step: 2140000. Time Elapsed: 36.766 s. Mean Reward: -0.477. Std of Reward: 1.349. Training.
[INFO] MoveAgent2D. Step: 2150000. Time Elapsed: 55.581 s. Mean Reward: -0.448. Std of Reward: 1.351. Training.
[INFO] MoveAgent2D. Step: 2160000. Time Elapsed: 64.088 s. Mean Reward: -0.276. Std of Reward: 1.323. Training.
[INFO] MoveAgent2D. Step: 2170000. Time Elapsed: 72.353 s. Mean Reward: -0.070. Std of Reward: 1.236. Training.
[INFO] MoveAgent2D. Step: 2180000. Time Elapsed: 81.036 s. Mean Reward: 0.042. Std of Reward: 1.191. Training.
[INFO] MoveAgent2D. Step: 2190000. Time Elapsed: 99.367 s. Mean Reward: 0.160. Std of Reward: 1.158. Training.
[INFO] MoveAgent2D. Step: 2200000. Time Elapsed: 107.782 s. Mean Reward: 0.154. Std of Reward: 1.138. Training.
[INFO] MoveAgent2D. Step: 2210000. Time Elapsed: 116.284 s. Mean Reward: -0.323. Std of Reward: 1.304. Training.
[INFO] MoveAgent2D. Step: 2220000. Time Elapsed: 124.671 s. Mean Reward: -0.131. Std of Reward: 1.287. Training.
[INFO] MoveAgent2D. Step: 2230000. Time Elapsed: 143.397 s. Mean Reward: 0.195. Std of Reward: 1.092. Training.
[INFO] MoveAgent2D. Step: 2240000. Time Elapsed: 151.920 s. Mean Reward: 0.144. Std of Reward: 1.142. Training.
[INFO] MoveAgent2D. Step: 2250000. Time Elapsed: 160.307 s. Mean Reward: -0.115. Std of Reward: 1.251. Training.
[INFO] MoveAgent2D. Step: 2260000. Time Elapsed: 168.709 s. Mean Reward: 0.065. Std of Reward: 1.144. Training.
[INFO] MoveAgent2D. Step: 2270000. Time Elapsed: 187.065 s. Mean Reward: 0.025. Std of Reward: 1.229. Training.
[INFO] MoveAgent2D. Step: 2280000. Time Elapsed: 195.803 s. Mean Reward: 0.050. Std of Reward: 1.167. Training.
[INFO] MoveAgent2D. Step: 2290000. Time Elapsed: 204.156 s. Mean Reward: 0.232. Std of Reward: 1.083. Training.
[INFO] MoveAgent2D. Step: 2300000. Time Elapsed: 212.558 s. Mean Reward: 0.344. Std of Reward: 0.964. Training.
[INFO] MoveAgent2D. Step: 2310000. Time Elapsed: 231.317 s. Mean Reward: 0.451. Std of Reward: 0.877. Training.
[INFO] MoveAgent2D. Step: 2320000. Time Elapsed: 239.678 s. Mean Reward: 0.478. Std of Reward: 0.866. Training.
[INFO] MoveAgent2D. Step: 2330000. Time Elapsed: 248.262 s. Mean Reward: 0.161. Std of Reward: 1.126. Training.
[INFO] MoveAgent2D. Step: 2340000. Time Elapsed: 256.674 s. Mean Reward: 0.381. Std of Reward: 0.981. Training.
[INFO] MoveAgent2D. Step: 2350000. Time Elapsed: 275.069 s. Mean Reward: 0.272. Std of Reward: 0.998. Training.
[INFO] MoveAgent2D. Step: 2360000. Time Elapsed: 283.755 s. Mean Reward: 0.446. Std of Reward: 0.896. Training.
[INFO] MoveAgent2D. Step: 2370000. Time Elapsed: 292.188 s. Mean Reward: 0.349. Std of Reward: 0.944. Training.
[INFO] MoveAgent2D. Step: 2380000. Time Elapsed: 300.517 s. Mean Reward: 0.486. Std of Reward: 0.870. Training.
[INFO] MoveAgent2D. Step: 2390000. Time Elapsed: 319.400 s. Mean Reward: 0.401. Std of Reward: 0.927. Training.
[INFO] MoveAgent2D. Step: 2400000. Time Elapsed: 327.904 s. Mean Reward: 0.352. Std of Reward: 0.990. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-2400080.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-2400080.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 8 - max_steps: 2700000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9575
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      2700000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 2400080.
[INFO] MoveAgent2D. Step: 2410000. Time Elapsed: 12.077 s. Mean Reward: 0.517. Std of Reward: 0.795. Training.
[INFO] MoveAgent2D. Step: 2420000. Time Elapsed: 20.490 s. Mean Reward: 0.552. Std of Reward: 0.708. Training.
[INFO] MoveAgent2D. Step: 2430000. Time Elapsed: 28.662 s. Mean Reward: 0.242. Std of Reward: 1.077. Training.
[INFO] MoveAgent2D. Step: 2440000. Time Elapsed: 37.348 s. Mean Reward: 0.137. Std of Reward: 1.190. Training.
[INFO] MoveAgent2D. Step: 2450000. Time Elapsed: 55.964 s. Mean Reward: 0.605. Std of Reward: 0.594. Training.
[INFO] MoveAgent2D. Step: 2460000. Time Elapsed: 64.653 s. Mean Reward: 0.518. Std of Reward: 0.788. Training.
[INFO] MoveAgent2D. Step: 2470000. Time Elapsed: 73.091 s. Mean Reward: 0.498. Std of Reward: 0.823. Training.
[INFO] MoveAgent2D. Step: 2480000. Time Elapsed: 81.606 s. Mean Reward: 0.280. Std of Reward: 1.100. Training.
[INFO] MoveAgent2D. Step: 2490000. Time Elapsed: 100.404 s. Mean Reward: 0.416. Std of Reward: 0.971. Training.
[INFO] MoveAgent2D. Step: 2500000. Time Elapsed: 109.050 s. Mean Reward: 0.349. Std of Reward: 0.967. Training.
[INFO] MoveAgent2D. Step: 2510000. Time Elapsed: 117.498 s. Mean Reward: 0.513. Std of Reward: 0.820. Training.
[INFO] MoveAgent2D. Step: 2520000. Time Elapsed: 125.957 s. Mean Reward: 0.411. Std of Reward: 0.881. Training.
[INFO] MoveAgent2D. Step: 2530000. Time Elapsed: 144.553 s. Mean Reward: 0.493. Std of Reward: 0.828. Training.
[INFO] MoveAgent2D. Step: 2540000. Time Elapsed: 153.092 s. Mean Reward: 0.298. Std of Reward: 1.021. Training.
[INFO] MoveAgent2D. Step: 2550000. Time Elapsed: 161.439 s. Mean Reward: 0.530. Std of Reward: 0.801. Training.
[INFO] MoveAgent2D. Step: 2560000. Time Elapsed: 169.847 s. Mean Reward: 0.427. Std of Reward: 0.866. Training.
[INFO] MoveAgent2D. Step: 2570000. Time Elapsed: 188.819 s. Mean Reward: 0.537. Std of Reward: 0.781. Training.
[INFO] MoveAgent2D. Step: 2580000. Time Elapsed: 197.450 s. Mean Reward: 0.695. Std of Reward: 0.518. Training.
[INFO] MoveAgent2D. Step: 2590000. Time Elapsed: 205.954 s. Mean Reward: 0.480. Std of Reward: 0.849. Training.
[INFO] MoveAgent2D. Step: 2600000. Time Elapsed: 214.302 s. Mean Reward: 0.405. Std of Reward: 0.951. Training.
[INFO] MoveAgent2D. Step: 2610000. Time Elapsed: 233.462 s. Mean Reward: 0.497. Std of Reward: 0.865. Training.
[INFO] MoveAgent2D. Step: 2620000. Time Elapsed: 241.744 s. Mean Reward: 0.396. Std of Reward: 0.874. Training.
[INFO] MoveAgent2D. Step: 2630000. Time Elapsed: 250.651 s. Mean Reward: 0.637. Std of Reward: 0.664. Training.
[INFO] MoveAgent2D. Step: 2640000. Time Elapsed: 259.369 s. Mean Reward: 0.725. Std of Reward: 0.490. Training.
[INFO] MoveAgent2D. Step: 2650000. Time Elapsed: 278.082 s. Mean Reward: 0.335. Std of Reward: 1.032. Training.
[INFO] MoveAgent2D. Step: 2660000. Time Elapsed: 286.732 s. Mean Reward: 0.566. Std of Reward: 0.734. Training.
[INFO] MoveAgent2D. Step: 2670000. Time Elapsed: 294.946 s. Mean Reward: 0.556. Std of Reward: 0.773. Training.
[INFO] MoveAgent2D. Step: 2680000. Time Elapsed: 303.553 s. Mean Reward: 0.596. Std of Reward: 0.683. Training.
[INFO] MoveAgent2D. Step: 2690000. Time Elapsed: 322.353 s. Mean Reward: 0.605. Std of Reward: 0.671. Training.
[INFO] MoveAgent2D. Step: 2700000. Time Elapsed: 330.965 s. Mean Reward: 0.590. Std of Reward: 0.705. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-2700030.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-2700030.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 9 - max_steps: 3000000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.96
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      3000000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 2700030.
[INFO] MoveAgent2D. Step: 2710000. Time Elapsed: 12.487 s. Mean Reward: 0.707. Std of Reward: 0.554. Training.
[INFO] MoveAgent2D. Step: 2720000. Time Elapsed: 20.647 s. Mean Reward: 0.580. Std of Reward: 0.652. Training.
[INFO] MoveAgent2D. Step: 2730000. Time Elapsed: 28.972 s. Mean Reward: 0.535. Std of Reward: 0.755. Training.
[INFO] MoveAgent2D. Step: 2740000. Time Elapsed: 37.990 s. Mean Reward: 0.617. Std of Reward: 0.655. Training.
[INFO] MoveAgent2D. Step: 2750000. Time Elapsed: 55.917 s. Mean Reward: 0.519. Std of Reward: 0.818. Training.
[INFO] MoveAgent2D. Step: 2760000. Time Elapsed: 63.288 s. Mean Reward: 0.618. Std of Reward: 0.600. Training.
[INFO] MoveAgent2D. Step: 2770000. Time Elapsed: 71.219 s. Mean Reward: 0.740. Std of Reward: 0.424. Training.
[INFO] MoveAgent2D. Step: 2780000. Time Elapsed: 78.505 s. Mean Reward: 0.499. Std of Reward: 0.823. Training.
[INFO] MoveAgent2D. Step: 2790000. Time Elapsed: 96.216 s. Mean Reward: 0.557. Std of Reward: 0.761. Training.
[INFO] MoveAgent2D. Step: 2800000. Time Elapsed: 103.874 s. Mean Reward: 0.592. Std of Reward: 0.751. Training.
[INFO] MoveAgent2D. Step: 2810000. Time Elapsed: 111.418 s. Mean Reward: 0.579. Std of Reward: 0.766. Training.
[INFO] MoveAgent2D. Step: 2820000. Time Elapsed: 119.255 s. Mean Reward: 0.694. Std of Reward: 0.580. Training.
[INFO] MoveAgent2D. Step: 2830000. Time Elapsed: 137.038 s. Mean Reward: 0.688. Std of Reward: 0.529. Training.
[INFO] MoveAgent2D. Step: 2840000. Time Elapsed: 144.656 s. Mean Reward: 0.751. Std of Reward: 0.410. Training.
[INFO] MoveAgent2D. Step: 2850000. Time Elapsed: 152.314 s. Mean Reward: 0.542. Std of Reward: 0.773. Training.
[INFO] MoveAgent2D. Step: 2860000. Time Elapsed: 159.855 s. Mean Reward: 0.709. Std of Reward: 0.486. Training.
[INFO] MoveAgent2D. Step: 2870000. Time Elapsed: 177.570 s. Mean Reward: 0.562. Std of Reward: 0.732. Training.
[INFO] MoveAgent2D. Step: 2880000. Time Elapsed: 185.109 s. Mean Reward: 0.707. Std of Reward: 0.512. Training.
[INFO] MoveAgent2D. Step: 2890000. Time Elapsed: 192.687 s. Mean Reward: 0.645. Std of Reward: 0.664. Training.
[INFO] MoveAgent2D. Step: 2900000. Time Elapsed: 200.488 s. Mean Reward: 0.678. Std of Reward: 0.608. Training.
[INFO] MoveAgent2D. Step: 2910000. Time Elapsed: 217.920 s. Mean Reward: 0.756. Std of Reward: 0.411. Training.
[INFO] MoveAgent2D. Step: 2920000. Time Elapsed: 225.829 s. Mean Reward: 0.742. Std of Reward: 0.467. Training.
[INFO] MoveAgent2D. Step: 2930000. Time Elapsed: 233.287 s. Mean Reward: 0.640. Std of Reward: 0.681. Training.
[INFO] MoveAgent2D. Step: 2940000. Time Elapsed: 240.857 s. Mean Reward: 0.661. Std of Reward: 0.623. Training.
[INFO] MoveAgent2D. Step: 2950000. Time Elapsed: 258.616 s. Mean Reward: 0.755. Std of Reward: 0.400. Training.
[INFO] MoveAgent2D. Step: 2960000. Time Elapsed: 266.194 s. Mean Reward: 0.690. Std of Reward: 0.444. Training.
[INFO] MoveAgent2D. Step: 2970000. Time Elapsed: 273.891 s. Mean Reward: 0.655. Std of Reward: 0.663. Training.
[INFO] MoveAgent2D. Step: 2980000. Time Elapsed: 281.359 s. Mean Reward: 0.779. Std of Reward: 0.382. Training.
[INFO] MoveAgent2D. Step: 2990000. Time Elapsed: 299.307 s. Mean Reward: 0.787. Std of Reward: 0.187. Training.
[INFO] MoveAgent2D. Step: 3000000. Time Elapsed: 306.798 s. Mean Reward: 0.628. Std of Reward: 0.649. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-2999979.onnx
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-3000035.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-3000035.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 10 - max_steps: 3300000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9625
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      3300000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 3000035.
[INFO] MoveAgent2D. Step: 3010000. Time Elapsed: 12.330 s. Mean Reward: 0.783. Std of Reward: 0.204. Training.
[INFO] MoveAgent2D. Step: 3020000. Time Elapsed: 20.403 s. Mean Reward: 0.790. Std of Reward: 0.299. Training.
[INFO] MoveAgent2D. Step: 3030000. Time Elapsed: 29.124 s. Mean Reward: 0.726. Std of Reward: 0.476. Training.
[INFO] MoveAgent2D. Step: 3040000. Time Elapsed: 37.884 s. Mean Reward: 0.798. Std of Reward: 0.293. Training.
[INFO] MoveAgent2D. Step: 3050000. Time Elapsed: 57.132 s. Mean Reward: 0.764. Std of Reward: 0.396. Training.
[INFO] MoveAgent2D. Step: 3060000. Time Elapsed: 65.596 s. Mean Reward: 0.749. Std of Reward: 0.474. Training.
[INFO] MoveAgent2D. Step: 3070000. Time Elapsed: 74.022 s. Mean Reward: 0.721. Std of Reward: 0.536. Training.
[INFO] MoveAgent2D. Step: 3080000. Time Elapsed: 82.813 s. Mean Reward: 0.762. Std of Reward: 0.323. Training.
[INFO] MoveAgent2D. Step: 3090000. Time Elapsed: 101.498 s. Mean Reward: 0.765. Std of Reward: 0.398. Training.
[INFO] MoveAgent2D. Step: 3100000. Time Elapsed: 110.054 s. Mean Reward: 0.734. Std of Reward: 0.484. Training.
[INFO] MoveAgent2D. Step: 3110000. Time Elapsed: 118.793 s. Mean Reward: 0.797. Std of Reward: 0.303. Training.
[INFO] MoveAgent2D. Step: 3120000. Time Elapsed: 127.590 s. Mean Reward: 0.747. Std of Reward: 0.509. Training.
[INFO] MoveAgent2D. Step: 3130000. Time Elapsed: 146.682 s. Mean Reward: 0.756. Std of Reward: 0.404. Training.
[INFO] MoveAgent2D. Step: 3140000. Time Elapsed: 155.314 s. Mean Reward: 0.837. Std of Reward: 0.131. Training.
[INFO] MoveAgent2D. Step: 3150000. Time Elapsed: 164.043 s. Mean Reward: 0.809. Std of Reward: 0.187. Training.
[INFO] MoveAgent2D. Step: 3160000. Time Elapsed: 172.680 s. Mean Reward: 0.778. Std of Reward: 0.382. Training.
[INFO] MoveAgent2D. Step: 3170000. Time Elapsed: 191.964 s. Mean Reward: 0.824. Std of Reward: 0.158. Training.
[INFO] MoveAgent2D. Step: 3180000. Time Elapsed: 200.465 s. Mean Reward: 0.761. Std of Reward: 0.398. Training.
[INFO] MoveAgent2D. Step: 3190000. Time Elapsed: 209.101 s. Mean Reward: 0.803. Std of Reward: 0.291. Training.
[INFO] MoveAgent2D. Step: 3200000. Time Elapsed: 217.847 s. Mean Reward: 0.818. Std of Reward: 0.169. Training.
[INFO] MoveAgent2D. Step: 3210000. Time Elapsed: 236.717 s. Mean Reward: 0.701. Std of Reward: 0.579. Training.
[INFO] MoveAgent2D. Step: 3220000. Time Elapsed: 245.468 s. Mean Reward: 0.833. Std of Reward: 0.257. Training.
[INFO] MoveAgent2D. Step: 3230000. Time Elapsed: 254.169 s. Mean Reward: 0.818. Std of Reward: 0.270. Training.
[INFO] MoveAgent2D. Step: 3240000. Time Elapsed: 262.875 s. Mean Reward: 0.812. Std of Reward: 0.287. Training.
[INFO] MoveAgent2D. Step: 3250000. Time Elapsed: 281.776 s. Mean Reward: 0.798. Std of Reward: 0.359. Training.
[INFO] MoveAgent2D. Step: 3260000. Time Elapsed: 290.293 s. Mean Reward: 0.818. Std of Reward: 0.275. Training.
[INFO] MoveAgent2D. Step: 3270000. Time Elapsed: 299.098 s. Mean Reward: 0.822. Std of Reward: 0.271. Training.
[INFO] MoveAgent2D. Step: 3280000. Time Elapsed: 307.768 s. Mean Reward: 0.805. Std of Reward: 0.277. Training.
[INFO] MoveAgent2D. Step: 3290000. Time Elapsed: 326.582 s. Mean Reward: 0.802. Std of Reward: 0.290. Training.
[INFO] MoveAgent2D. Step: 3300000. Time Elapsed: 335.390 s. Mean Reward: 0.844. Std of Reward: 0.151. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-3300078.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-3300078.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 11 - max_steps: 3600000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.965
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      3600000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 3300078.
[INFO] MoveAgent2D. Step: 3310000. Time Elapsed: 12.186 s. Mean Reward: 0.830. Std of Reward: 0.120. Training.
[INFO] MoveAgent2D. Step: 3320000. Time Elapsed: 20.276 s. Mean Reward: 0.832. Std of Reward: 0.267. Training.
[INFO] MoveAgent2D. Step: 3330000. Time Elapsed: 29.176 s. Mean Reward: 0.841. Std of Reward: 0.139. Training.
[INFO] MoveAgent2D. Step: 3340000. Time Elapsed: 37.606 s. Mean Reward: 0.837. Std of Reward: 0.136. Training.
[INFO] MoveAgent2D. Step: 3350000. Time Elapsed: 56.713 s. Mean Reward: 0.817. Std of Reward: 0.170. Training.
[INFO] MoveAgent2D. Step: 3360000. Time Elapsed: 65.479 s. Mean Reward: 0.820. Std of Reward: 0.266. Training.
[INFO] MoveAgent2D. Step: 3370000. Time Elapsed: 74.199 s. Mean Reward: 0.825. Std of Reward: 0.152. Training.
[INFO] MoveAgent2D. Step: 3380000. Time Elapsed: 82.631 s. Mean Reward: 0.795. Std of Reward: 0.371. Training.
[INFO] MoveAgent2D. Step: 3390000. Time Elapsed: 101.525 s. Mean Reward: 0.827. Std of Reward: 0.146. Training.
[INFO] MoveAgent2D. Step: 3400000. Time Elapsed: 110.325 s. Mean Reward: 0.836. Std of Reward: 0.136. Training.
[INFO] MoveAgent2D. Step: 3410000. Time Elapsed: 119.333 s. Mean Reward: 0.818. Std of Reward: 0.330. Training.
[INFO] MoveAgent2D. Step: 3420000. Time Elapsed: 128.110 s. Mean Reward: 0.842. Std of Reward: 0.134. Training.
[INFO] MoveAgent2D. Step: 3430000. Time Elapsed: 146.898 s. Mean Reward: 0.843. Std of Reward: 0.136. Training.
[INFO] MoveAgent2D. Step: 3440000. Time Elapsed: 155.967 s. Mean Reward: 0.832. Std of Reward: 0.265. Training.
[INFO] MoveAgent2D. Step: 3450000. Time Elapsed: 164.729 s. Mean Reward: 0.840. Std of Reward: 0.131. Training.
[INFO] MoveAgent2D. Step: 3460000. Time Elapsed: 173.167 s. Mean Reward: 0.838. Std of Reward: 0.137. Training.
[INFO] MoveAgent2D. Step: 3470000. Time Elapsed: 192.394 s. Mean Reward: 0.835. Std of Reward: 0.155. Training.
[INFO] MoveAgent2D. Step: 3480000. Time Elapsed: 200.862 s. Mean Reward: 0.820. Std of Reward: 0.271. Training.
[INFO] MoveAgent2D. Step: 3490000. Time Elapsed: 209.750 s. Mean Reward: 0.848. Std of Reward: 0.148. Training.
[INFO] MoveAgent2D. Step: 3500000. Time Elapsed: 218.475 s. Mean Reward: 0.848. Std of Reward: 0.125. Training.
[INFO] MoveAgent2D. Step: 3510000. Time Elapsed: 237.318 s. Mean Reward: 0.841. Std of Reward: 0.153. Training.
[INFO] MoveAgent2D. Step: 3520000. Time Elapsed: 245.977 s. Mean Reward: 0.847. Std of Reward: 0.129. Training.
[INFO] MoveAgent2D. Step: 3530000. Time Elapsed: 254.944 s. Mean Reward: 0.843. Std of Reward: 0.238. Training.
[INFO] MoveAgent2D. Step: 3540000. Time Elapsed: 263.678 s. Mean Reward: 0.799. Std of Reward: 0.291. Training.
[INFO] MoveAgent2D. Step: 3550000. Time Elapsed: 282.669 s. Mean Reward: 0.833. Std of Reward: 0.258. Training.
[INFO] MoveAgent2D. Step: 3560000. Time Elapsed: 291.395 s. Mean Reward: 0.851. Std of Reward: 0.137. Training.
[INFO] MoveAgent2D. Step: 3570000. Time Elapsed: 300.103 s. Mean Reward: 0.840. Std of Reward: 0.151. Training.
[INFO] MoveAgent2D. Step: 3580000. Time Elapsed: 308.765 s. Mean Reward: 0.849. Std of Reward: 0.120. Training.
[INFO] MoveAgent2D. Step: 3590000. Time Elapsed: 328.216 s. Mean Reward: 0.851. Std of Reward: 0.138. Training.
[INFO] MoveAgent2D. Step: 3600000. Time Elapsed: 336.725 s. Mean Reward: 0.635. Std of Reward: 0.747. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-3600001.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-3600001.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 12 - max_steps: 3900000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9675
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      3900000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 3600001.
[INFO] MoveAgent2D. Step: 3610000. Time Elapsed: 12.562 s. Mean Reward: 0.685. Std of Reward: 0.629. Training.
[INFO] MoveAgent2D. Step: 3620000. Time Elapsed: 20.751 s. Mean Reward: 0.660. Std of Reward: 0.658. Training.
[INFO] MoveAgent2D. Step: 3630000. Time Elapsed: 28.988 s. Mean Reward: 0.633. Std of Reward: 0.704. Training.
[INFO] MoveAgent2D. Step: 3640000. Time Elapsed: 37.732 s. Mean Reward: 0.616. Std of Reward: 0.806. Training.
[INFO] MoveAgent2D. Step: 3650000. Time Elapsed: 56.663 s. Mean Reward: 0.753. Std of Reward: 0.419. Training.
[INFO] MoveAgent2D. Step: 3660000. Time Elapsed: 65.083 s. Mean Reward: 0.566. Std of Reward: 0.822. Training.
[INFO] MoveAgent2D. Step: 3670000. Time Elapsed: 73.563 s. Mean Reward: 0.633. Std of Reward: 0.717. Training.
[INFO] MoveAgent2D. Step: 3680000. Time Elapsed: 82.255 s. Mean Reward: 0.756. Std of Reward: 0.510. Training.
[INFO] MoveAgent2D. Step: 3690000. Time Elapsed: 101.241 s. Mean Reward: 0.744. Std of Reward: 0.558. Training.
[INFO] MoveAgent2D. Step: 3700000. Time Elapsed: 109.734 s. Mean Reward: 0.642. Std of Reward: 0.675. Training.
[INFO] MoveAgent2D. Step: 3710000. Time Elapsed: 118.519 s. Mean Reward: 0.710. Std of Reward: 0.622. Training.
[INFO] MoveAgent2D. Step: 3720000. Time Elapsed: 127.312 s. Mean Reward: 0.775. Std of Reward: 0.443. Training.
[INFO] MoveAgent2D. Step: 3730000. Time Elapsed: 145.914 s. Mean Reward: 0.622. Std of Reward: 0.771. Training.
[INFO] MoveAgent2D. Step: 3740000. Time Elapsed: 154.425 s. Mean Reward: 0.721. Std of Reward: 0.547. Training.
[INFO] MoveAgent2D. Step: 3750000. Time Elapsed: 163.146 s. Mean Reward: 0.726. Std of Reward: 0.476. Training.
[INFO] MoveAgent2D. Step: 3760000. Time Elapsed: 171.694 s. Mean Reward: 0.739. Std of Reward: 0.487. Training.
[INFO] MoveAgent2D. Step: 3770000. Time Elapsed: 190.231 s. Mean Reward: 0.746. Std of Reward: 0.485. Training.
[INFO] MoveAgent2D. Step: 3780000. Time Elapsed: 198.988 s. Mean Reward: 0.580. Std of Reward: 0.814. Training.
[INFO] MoveAgent2D. Step: 3790000. Time Elapsed: 207.682 s. Mean Reward: 0.664. Std of Reward: 0.657. Training.
[INFO] MoveAgent2D. Step: 3800000. Time Elapsed: 216.203 s. Mean Reward: 0.751. Std of Reward: 0.470. Training.
[INFO] MoveAgent2D. Step: 3810000. Time Elapsed: 234.763 s. Mean Reward: 0.830. Std of Reward: 0.273. Training.
[INFO] MoveAgent2D. Step: 3820000. Time Elapsed: 243.567 s. Mean Reward: 0.792. Std of Reward: 0.427. Training.
[INFO] MoveAgent2D. Step: 3830000. Time Elapsed: 252.135 s. Mean Reward: 0.780. Std of Reward: 0.396. Training.
[INFO] MoveAgent2D. Step: 3840000. Time Elapsed: 260.874 s. Mean Reward: 0.713. Std of Reward: 0.573. Training.
[INFO] MoveAgent2D. Step: 3850000. Time Elapsed: 279.824 s. Mean Reward: 0.819. Std of Reward: 0.294. Training.
[INFO] MoveAgent2D. Step: 3860000. Time Elapsed: 288.259 s. Mean Reward: 0.744. Std of Reward: 0.550. Training.
[INFO] MoveAgent2D. Step: 3870000. Time Elapsed: 297.239 s. Mean Reward: 0.797. Std of Reward: 0.367. Training.
[INFO] MoveAgent2D. Step: 3880000. Time Elapsed: 305.953 s. Mean Reward: 0.824. Std of Reward: 0.342. Training.
[INFO] MoveAgent2D. Step: 3890000. Time Elapsed: 324.947 s. Mean Reward: 0.798. Std of Reward: 0.426. Training.
[INFO] MoveAgent2D. Step: 3900000. Time Elapsed: 333.716 s. Mean Reward: 0.766. Std of Reward: 0.513. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-3900069.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-3900069.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 13 - max_steps: 4200000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.97
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      4200000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 3900069.
[INFO] MoveAgent2D. Step: 3910000. Time Elapsed: 12.233 s. Mean Reward: 0.765. Std of Reward: 0.511. Training.
[INFO] MoveAgent2D. Step: 3920000. Time Elapsed: 20.489 s. Mean Reward: 0.778. Std of Reward: 0.438. Training.
[INFO] MoveAgent2D. Step: 3930000. Time Elapsed: 29.319 s. Mean Reward: 0.804. Std of Reward: 0.363. Training.
[INFO] MoveAgent2D. Step: 3940000. Time Elapsed: 38.250 s. Mean Reward: 0.804. Std of Reward: 0.368. Training.
[INFO] MoveAgent2D. Step: 3950000. Time Elapsed: 56.939 s. Mean Reward: 0.741. Std of Reward: 0.558. Training.
[INFO] MoveAgent2D. Step: 3960000. Time Elapsed: 65.757 s. Mean Reward: 0.807. Std of Reward: 0.350. Training.
[INFO] MoveAgent2D. Step: 3970000. Time Elapsed: 74.139 s. Mean Reward: 0.784. Std of Reward: 0.440. Training.
[INFO] MoveAgent2D. Step: 3980000. Time Elapsed: 82.304 s. Mean Reward: 0.834. Std of Reward: 0.173. Training.
[INFO] MoveAgent2D. Step: 3990000. Time Elapsed: 100.216 s. Mean Reward: 0.784. Std of Reward: 0.464. Training.
[INFO] MoveAgent2D. Step: 4000000. Time Elapsed: 107.873 s. Mean Reward: 0.817. Std of Reward: 0.288. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-3999999.onnx
[INFO] MoveAgent2D. Step: 4010000. Time Elapsed: 116.008 s. Mean Reward: 0.829. Std of Reward: 0.182. Training.
[INFO] MoveAgent2D. Step: 4020000. Time Elapsed: 123.618 s. Mean Reward: 0.832. Std of Reward: 0.153. Training.
[INFO] MoveAgent2D. Step: 4030000. Time Elapsed: 141.760 s. Mean Reward: 0.850. Std of Reward: 0.135. Training.
[INFO] MoveAgent2D. Step: 4040000. Time Elapsed: 149.788 s. Mean Reward: 0.850. Std of Reward: 0.161. Training.
[INFO] MoveAgent2D. Step: 4050000. Time Elapsed: 157.629 s. Mean Reward: 0.819. Std of Reward: 0.296. Training.
[INFO] MoveAgent2D. Step: 4060000. Time Elapsed: 165.835 s. Mean Reward: 0.812. Std of Reward: 0.398. Training.
[INFO] MoveAgent2D. Step: 4070000. Time Elapsed: 183.926 s. Mean Reward: 0.853. Std of Reward: 0.143. Training.
[INFO] MoveAgent2D. Step: 4080000. Time Elapsed: 191.910 s. Mean Reward: 0.842. Std of Reward: 0.248. Training.
[INFO] MoveAgent2D. Step: 4090000. Time Elapsed: 199.676 s. Mean Reward: 0.831. Std of Reward: 0.267. Training.
[INFO] MoveAgent2D. Step: 4100000. Time Elapsed: 207.717 s. Mean Reward: 0.838. Std of Reward: 0.258. Training.
[INFO] MoveAgent2D. Step: 4110000. Time Elapsed: 225.893 s. Mean Reward: 0.834. Std of Reward: 0.323. Training.
[INFO] MoveAgent2D. Step: 4120000. Time Elapsed: 233.971 s. Mean Reward: 0.868. Std of Reward: 0.113. Training.
[INFO] MoveAgent2D. Step: 4130000. Time Elapsed: 241.790 s. Mean Reward: 0.822. Std of Reward: 0.274. Training.
[INFO] MoveAgent2D. Step: 4140000. Time Elapsed: 249.757 s. Mean Reward: 0.863. Std of Reward: 0.134. Training.
[INFO] MoveAgent2D. Step: 4150000. Time Elapsed: 267.861 s. Mean Reward: 0.856. Std of Reward: 0.132. Training.
[INFO] MoveAgent2D. Step: 4160000. Time Elapsed: 275.825 s. Mean Reward: 0.847. Std of Reward: 0.120. Training.
[INFO] MoveAgent2D. Step: 4170000. Time Elapsed: 284.348 s. Mean Reward: 0.863. Std of Reward: 0.125. Training.
[INFO] MoveAgent2D. Step: 4180000. Time Elapsed: 292.135 s. Mean Reward: 0.827. Std of Reward: 0.280. Training.
[INFO] MoveAgent2D. Step: 4190000. Time Elapsed: 310.090 s. Mean Reward: 0.845. Std of Reward: 0.149. Training.
[INFO] MoveAgent2D. Step: 4200000. Time Elapsed: 318.023 s. Mean Reward: 0.857. Std of Reward: 0.136. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-4200024.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-4200024.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 14 - max_steps: 4500000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9725
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      4500000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 4200024.
[INFO] MoveAgent2D. Step: 4210000. Time Elapsed: 12.429 s. Mean Reward: 0.869. Std of Reward: 0.110. Training.
[INFO] MoveAgent2D. Step: 4220000. Time Elapsed: 21.040 s. Mean Reward: 0.874. Std of Reward: 0.117. Training.
[INFO] MoveAgent2D. Step: 4230000. Time Elapsed: 30.001 s. Mean Reward: 0.872. Std of Reward: 0.128. Training.
[INFO] MoveAgent2D. Step: 4240000. Time Elapsed: 38.696 s. Mean Reward: 0.862. Std of Reward: 0.122. Training.
[INFO] MoveAgent2D. Step: 4250000. Time Elapsed: 58.053 s. Mean Reward: 0.878. Std of Reward: 0.123. Training.
[INFO] MoveAgent2D. Step: 4260000. Time Elapsed: 66.682 s. Mean Reward: 0.866. Std of Reward: 0.122. Training.
[INFO] MoveAgent2D. Step: 4270000. Time Elapsed: 76.030 s. Mean Reward: 0.869. Std of Reward: 0.117. Training.
[INFO] MoveAgent2D. Step: 4280000. Time Elapsed: 84.689 s. Mean Reward: 0.865. Std of Reward: 0.117. Training.
[INFO] MoveAgent2D. Step: 4290000. Time Elapsed: 103.784 s. Mean Reward: 0.864. Std of Reward: 0.126. Training.
[INFO] MoveAgent2D. Step: 4300000. Time Elapsed: 112.836 s. Mean Reward: 0.872. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 4310000. Time Elapsed: 121.659 s. Mean Reward: 0.866. Std of Reward: 0.116. Training.
[INFO] MoveAgent2D. Step: 4320000. Time Elapsed: 130.494 s. Mean Reward: 0.877. Std of Reward: 0.114. Training.
[INFO] MoveAgent2D. Step: 4330000. Time Elapsed: 149.956 s. Mean Reward: 0.866. Std of Reward: 0.125. Training.
[INFO] MoveAgent2D. Step: 4340000. Time Elapsed: 158.830 s. Mean Reward: 0.858. Std of Reward: 0.140. Training.
[INFO] MoveAgent2D. Step: 4350000. Time Elapsed: 167.684 s. Mean Reward: 0.867. Std of Reward: 0.101. Training.
[INFO] MoveAgent2D. Step: 4360000. Time Elapsed: 176.912 s. Mean Reward: 0.883. Std of Reward: 0.104. Training.
[INFO] MoveAgent2D. Step: 4370000. Time Elapsed: 196.372 s. Mean Reward: 0.875. Std of Reward: 0.105. Training.
[INFO] MoveAgent2D. Step: 4380000. Time Elapsed: 205.261 s. Mean Reward: 0.863. Std of Reward: 0.238. Training.
[INFO] MoveAgent2D. Step: 4390000. Time Elapsed: 214.145 s. Mean Reward: 0.878. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 4400000. Time Elapsed: 223.228 s. Mean Reward: 0.883. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 4410000. Time Elapsed: 242.587 s. Mean Reward: 0.866. Std of Reward: 0.225. Training.
[INFO] MoveAgent2D. Step: 4420000. Time Elapsed: 251.355 s. Mean Reward: 0.849. Std of Reward: 0.244. Training.
[INFO] MoveAgent2D. Step: 4430000. Time Elapsed: 260.149 s. Mean Reward: 0.869. Std of Reward: 0.103. Training.
[INFO] MoveAgent2D. Step: 4440000. Time Elapsed: 269.260 s. Mean Reward: 0.883. Std of Reward: 0.105. Training.
[INFO] MoveAgent2D. Step: 4450000. Time Elapsed: 288.735 s. Mean Reward: 0.884. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 4460000. Time Elapsed: 297.634 s. Mean Reward: 0.869. Std of Reward: 0.109. Training.
[INFO] MoveAgent2D. Step: 4470000. Time Elapsed: 306.705 s. Mean Reward: 0.883. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 4480000. Time Elapsed: 315.554 s. Mean Reward: 0.853. Std of Reward: 0.239. Training.
[INFO] MoveAgent2D. Step: 4490000. Time Elapsed: 334.898 s. Mean Reward: 0.876. Std of Reward: 0.118. Training.
[INFO] MoveAgent2D. Step: 4500000. Time Elapsed: 343.923 s. Mean Reward: 0.888. Std of Reward: 0.089. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-4500001.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-4500001.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 15 - max_steps: 4800000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.975
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      4800000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 4500001.
[INFO] MoveAgent2D. Step: 4510000. Time Elapsed: 12.400 s. Mean Reward: 0.880. Std of Reward: 0.105. Training.
[INFO] MoveAgent2D. Step: 4520000. Time Elapsed: 21.121 s. Mean Reward: 0.878. Std of Reward: 0.110. Training.
[INFO] MoveAgent2D. Step: 4530000. Time Elapsed: 30.003 s. Mean Reward: 0.880. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 4540000. Time Elapsed: 38.821 s. Mean Reward: 0.877. Std of Reward: 0.101. Training.
[INFO] MoveAgent2D. Step: 4550000. Time Elapsed: 58.056 s. Mean Reward: 0.876. Std of Reward: 0.110. Training.
[INFO] MoveAgent2D. Step: 4560000. Time Elapsed: 66.970 s. Mean Reward: 0.878. Std of Reward: 0.093. Training.
[INFO] MoveAgent2D. Step: 4570000. Time Elapsed: 75.991 s. Mean Reward: 0.880. Std of Reward: 0.094. Training.
[INFO] MoveAgent2D. Step: 4580000. Time Elapsed: 84.929 s. Mean Reward: 0.884. Std of Reward: 0.103. Training.
[INFO] MoveAgent2D. Step: 4590000. Time Elapsed: 104.165 s. Mean Reward: 0.888. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 4600000. Time Elapsed: 113.079 s. Mean Reward: 0.884. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 4610000. Time Elapsed: 122.115 s. Mean Reward: 0.878. Std of Reward: 0.105. Training.
[INFO] MoveAgent2D. Step: 4620000. Time Elapsed: 131.219 s. Mean Reward: 0.888. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 4630000. Time Elapsed: 150.284 s. Mean Reward: 0.728. Std of Reward: 0.643. Training.
[INFO] MoveAgent2D. Step: 4640000. Time Elapsed: 158.584 s. Mean Reward: -0.071. Std of Reward: 1.325. Training.
[INFO] MoveAgent2D. Step: 4650000. Time Elapsed: 166.965 s. Mean Reward: 0.001. Std of Reward: 1.310. Training.
[INFO] MoveAgent2D. Step: 4660000. Time Elapsed: 175.351 s. Mean Reward: -0.065. Std of Reward: 1.293. Training.
[INFO] MoveAgent2D. Step: 4670000. Time Elapsed: 193.992 s. Mean Reward: 0.546. Std of Reward: 0.858. Training.
[INFO] MoveAgent2D. Step: 4680000. Time Elapsed: 202.530 s. Mean Reward: 0.687. Std of Reward: 0.679. Training.
[INFO] MoveAgent2D. Step: 4690000. Time Elapsed: 211.070 s. Mean Reward: 0.673. Std of Reward: 0.695. Training.
[INFO] MoveAgent2D. Step: 4700000. Time Elapsed: 219.832 s. Mean Reward: 0.631. Std of Reward: 0.774. Training.
[INFO] MoveAgent2D. Step: 4710000. Time Elapsed: 238.803 s. Mean Reward: 0.712. Std of Reward: 0.593. Training.
[INFO] MoveAgent2D. Step: 4720000. Time Elapsed: 247.526 s. Mean Reward: 0.759. Std of Reward: 0.466. Training.
[INFO] MoveAgent2D. Step: 4730000. Time Elapsed: 256.176 s. Mean Reward: 0.537. Std of Reward: 0.850. Training.
[INFO] MoveAgent2D. Step: 4740000. Time Elapsed: 264.661 s. Mean Reward: 0.703. Std of Reward: 0.622. Training.
[INFO] MoveAgent2D. Step: 4750000. Time Elapsed: 283.606 s. Mean Reward: 0.759. Std of Reward: 0.458. Training.
[INFO] MoveAgent2D. Step: 4760000. Time Elapsed: 292.182 s. Mean Reward: 0.802. Std of Reward: 0.455. Training.
[INFO] MoveAgent2D. Step: 4770000. Time Elapsed: 301.187 s. Mean Reward: 0.832. Std of Reward: 0.330. Training.
[INFO] MoveAgent2D. Step: 4780000. Time Elapsed: 309.885 s. Mean Reward: 0.858. Std of Reward: 0.136. Training.
[INFO] MoveAgent2D. Step: 4790000. Time Elapsed: 328.932 s. Mean Reward: 0.822. Std of Reward: 0.274. Training.
[INFO] MoveAgent2D. Step: 4800000. Time Elapsed: 337.613 s. Mean Reward: 0.855. Std of Reward: 0.135. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-4800048.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-4800048.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 16 - max_steps: 5100000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9775
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      5100000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 4800048.
[INFO] MoveAgent2D. Step: 4810000. Time Elapsed: 12.368 s. Mean Reward: 0.867. Std of Reward: 0.112. Training.
[INFO] MoveAgent2D. Step: 4820000. Time Elapsed: 21.068 s. Mean Reward: 0.875. Std of Reward: 0.138. Training.
[INFO] MoveAgent2D. Step: 4830000. Time Elapsed: 30.026 s. Mean Reward: 0.865. Std of Reward: 0.129. Training.
[INFO] MoveAgent2D. Step: 4840000. Time Elapsed: 38.952 s. Mean Reward: 0.864. Std of Reward: 0.127. Training.
[INFO] MoveAgent2D. Step: 4850000. Time Elapsed: 57.363 s. Mean Reward: 0.865. Std of Reward: 0.115. Training.
[INFO] MoveAgent2D. Step: 4860000. Time Elapsed: 66.232 s. Mean Reward: 0.886. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 4870000. Time Elapsed: 75.397 s. Mean Reward: 0.858. Std of Reward: 0.237. Training.
[INFO] MoveAgent2D. Step: 4880000. Time Elapsed: 84.090 s. Mean Reward: 0.865. Std of Reward: 0.129. Training.
[INFO] MoveAgent2D. Step: 4890000. Time Elapsed: 102.950 s. Mean Reward: 0.881. Std of Reward: 0.099. Training.
[INFO] MoveAgent2D. Step: 4900000. Time Elapsed: 111.896 s. Mean Reward: 0.879. Std of Reward: 0.096. Training.
[INFO] MoveAgent2D. Step: 4910000. Time Elapsed: 120.505 s. Mean Reward: 0.862. Std of Reward: 0.115. Training.
[INFO] MoveAgent2D. Step: 4920000. Time Elapsed: 129.378 s. Mean Reward: 0.882. Std of Reward: 0.103. Training.
[INFO] MoveAgent2D. Step: 4930000. Time Elapsed: 148.572 s. Mean Reward: 0.872. Std of Reward: 0.104. Training.
[INFO] MoveAgent2D. Step: 4940000. Time Elapsed: 157.427 s. Mean Reward: 0.882. Std of Reward: 0.099. Training.
[INFO] MoveAgent2D. Step: 4950000. Time Elapsed: 166.336 s. Mean Reward: 0.886. Std of Reward: 0.096. Training.
[INFO] MoveAgent2D. Step: 4960000. Time Elapsed: 175.198 s. Mean Reward: 0.878. Std of Reward: 0.104. Training.
[INFO] MoveAgent2D. Step: 4970000. Time Elapsed: 193.784 s. Mean Reward: 0.874. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 4980000. Time Elapsed: 201.562 s. Mean Reward: 0.888. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 4990000. Time Elapsed: 209.490 s. Mean Reward: 0.884. Std of Reward: 0.092. Training.
[INFO] MoveAgent2D. Step: 5000000. Time Elapsed: 217.426 s. Mean Reward: 0.878. Std of Reward: 0.090. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-4999992.onnx
[INFO] MoveAgent2D. Step: 5010000. Time Elapsed: 235.762 s. Mean Reward: 0.887. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 5020000. Time Elapsed: 243.634 s. Mean Reward: 0.891. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 5030000. Time Elapsed: 251.417 s. Mean Reward: 0.889. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 5040000. Time Elapsed: 259.360 s. Mean Reward: 0.881. Std of Reward: 0.096. Training.
[INFO] MoveAgent2D. Step: 5050000. Time Elapsed: 277.122 s. Mean Reward: 0.887. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5060000. Time Elapsed: 285.003 s. Mean Reward: 0.888. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5070000. Time Elapsed: 292.784 s. Mean Reward: 0.879. Std of Reward: 0.094. Training.
[INFO] MoveAgent2D. Step: 5080000. Time Elapsed: 300.328 s. Mean Reward: 0.884. Std of Reward: 0.084. Training.
[INFO] MoveAgent2D. Step: 5090000. Time Elapsed: 318.180 s. Mean Reward: 0.881. Std of Reward: 0.098. Training.
[INFO] MoveAgent2D. Step: 5100000. Time Elapsed: 325.844 s. Mean Reward: 0.889. Std of Reward: 0.093. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-5100045.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-5100045.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 17 - max_steps: 5400000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.98
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      5400000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 5100045.
[INFO] MoveAgent2D. Step: 5110000. Time Elapsed: 12.562 s. Mean Reward: 0.889. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 5120000. Time Elapsed: 21.273 s. Mean Reward: 0.884. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 5130000. Time Elapsed: 30.303 s. Mean Reward: 0.884. Std of Reward: 0.084. Training.
[INFO] MoveAgent2D. Step: 5140000. Time Elapsed: 39.415 s. Mean Reward: 0.890. Std of Reward: 0.089. Training.
[INFO] MoveAgent2D. Step: 5150000. Time Elapsed: 58.610 s. Mean Reward: 0.887. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 5160000. Time Elapsed: 67.766 s. Mean Reward: 0.891. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5170000. Time Elapsed: 76.672 s. Mean Reward: 0.890. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 5180000. Time Elapsed: 85.874 s. Mean Reward: 0.895. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 5190000. Time Elapsed: 105.043 s. Mean Reward: 0.884. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5200000. Time Elapsed: 113.908 s. Mean Reward: 0.878. Std of Reward: 0.094. Training.
[INFO] MoveAgent2D. Step: 5210000. Time Elapsed: 123.176 s. Mean Reward: 0.897. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 5220000. Time Elapsed: 132.226 s. Mean Reward: 0.880. Std of Reward: 0.097. Training.
[INFO] MoveAgent2D. Step: 5230000. Time Elapsed: 151.387 s. Mean Reward: 0.889. Std of Reward: 0.089. Training.
[INFO] MoveAgent2D. Step: 5240000. Time Elapsed: 160.484 s. Mean Reward: 0.896. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 5250000. Time Elapsed: 169.724 s. Mean Reward: 0.883. Std of Reward: 0.093. Training.
[INFO] MoveAgent2D. Step: 5260000. Time Elapsed: 178.644 s. Mean Reward: 0.888. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 5270000. Time Elapsed: 197.789 s. Mean Reward: 0.891. Std of Reward: 0.093. Training.
[INFO] MoveAgent2D. Step: 5280000. Time Elapsed: 206.977 s. Mean Reward: 0.895. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 5290000. Time Elapsed: 216.084 s. Mean Reward: 0.885. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 5300000. Time Elapsed: 225.229 s. Mean Reward: 0.885. Std of Reward: 0.093. Training.
[INFO] MoveAgent2D. Step: 5310000. Time Elapsed: 243.961 s. Mean Reward: 0.891. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 5320000. Time Elapsed: 253.092 s. Mean Reward: 0.889. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 5330000. Time Elapsed: 262.344 s. Mean Reward: 0.890. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 5340000. Time Elapsed: 271.273 s. Mean Reward: 0.885. Std of Reward: 0.091. Training.
[INFO] MoveAgent2D. Step: 5350000. Time Elapsed: 290.495 s. Mean Reward: 0.890. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 5360000. Time Elapsed: 299.416 s. Mean Reward: 0.893. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5370000. Time Elapsed: 308.528 s. Mean Reward: 0.894. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 5380000. Time Elapsed: 317.704 s. Mean Reward: 0.902. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 5390000. Time Elapsed: 337.132 s. Mean Reward: 0.895. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 5400000. Time Elapsed: 346.047 s. Mean Reward: 0.882. Std of Reward: 0.095. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-5400022.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-5400022.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 18 - max_steps: 5700000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9825
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      5700000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 5400022.
[INFO] MoveAgent2D. Step: 5410000. Time Elapsed: 12.616 s. Mean Reward: 0.901. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 5420000. Time Elapsed: 21.631 s. Mean Reward: 0.890. Std of Reward: 0.089. Training.
[INFO] MoveAgent2D. Step: 5430000. Time Elapsed: 30.734 s. Mean Reward: 0.897. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 5440000. Time Elapsed: 39.989 s. Mean Reward: 0.896. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 5450000. Time Elapsed: 59.237 s. Mean Reward: 0.889. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5460000. Time Elapsed: 68.525 s. Mean Reward: 0.900. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 5470000. Time Elapsed: 77.664 s. Mean Reward: 0.895. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 5480000. Time Elapsed: 86.952 s. Mean Reward: 0.890. Std of Reward: 0.098. Training.
[INFO] MoveAgent2D. Step: 5490000. Time Elapsed: 106.348 s. Mean Reward: 0.896. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 5500000. Time Elapsed: 115.638 s. Mean Reward: 0.898. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 5510000. Time Elapsed: 124.831 s. Mean Reward: 0.891. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 5520000. Time Elapsed: 133.826 s. Mean Reward: 0.891. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 5530000. Time Elapsed: 153.574 s. Mean Reward: 0.895. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 5540000. Time Elapsed: 162.552 s. Mean Reward: 0.889. Std of Reward: 0.084. Training.
[INFO] MoveAgent2D. Step: 5550000. Time Elapsed: 171.601 s. Mean Reward: 0.892. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5560000. Time Elapsed: 180.838 s. Mean Reward: 0.897. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 5570000. Time Elapsed: 200.352 s. Mean Reward: 0.884. Std of Reward: 0.114. Training.
[INFO] MoveAgent2D. Step: 5580000. Time Elapsed: 209.240 s. Mean Reward: 0.883. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5590000. Time Elapsed: 218.300 s. Mean Reward: 0.890. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 5600000. Time Elapsed: 227.364 s. Mean Reward: 0.890. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 5610000. Time Elapsed: 246.911 s. Mean Reward: 0.885. Std of Reward: 0.094. Training.
[INFO] MoveAgent2D. Step: 5620000. Time Elapsed: 256.108 s. Mean Reward: 0.837. Std of Reward: 0.415. Training.
[INFO] MoveAgent2D. Step: 5630000. Time Elapsed: 264.938 s. Mean Reward: 0.833. Std of Reward: 0.426. Training.
[INFO] MoveAgent2D. Step: 5640000. Time Elapsed: 273.835 s. Mean Reward: 0.820. Std of Reward: 0.431. Training.
[INFO] MoveAgent2D. Step: 5650000. Time Elapsed: 293.667 s. Mean Reward: 0.866. Std of Reward: 0.239. Training.
[INFO] MoveAgent2D. Step: 5660000. Time Elapsed: 302.444 s. Mean Reward: 0.873. Std of Reward: 0.122. Training.
[INFO] MoveAgent2D. Step: 5670000. Time Elapsed: 311.735 s. Mean Reward: 0.897. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5680000. Time Elapsed: 320.787 s. Mean Reward: 0.871. Std of Reward: 0.219. Training.
[INFO] MoveAgent2D. Step: 5690000. Time Elapsed: 340.155 s. Mean Reward: 0.890. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 5700000. Time Elapsed: 349.191 s. Mean Reward: 0.884. Std of Reward: 0.092. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-5700003.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-5700003.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 19 - max_steps: 6000000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.985
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      6000000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 5700003.
[INFO] MoveAgent2D. Step: 5710000. Time Elapsed: 12.463 s. Mean Reward: 0.888. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5720000. Time Elapsed: 21.418 s. Mean Reward: 0.897. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5730000. Time Elapsed: 30.715 s. Mean Reward: 0.895. Std of Reward: 0.084. Training.
[INFO] MoveAgent2D. Step: 5740000. Time Elapsed: 39.752 s. Mean Reward: 0.890. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 5750000. Time Elapsed: 58.987 s. Mean Reward: 0.886. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 5760000. Time Elapsed: 68.015 s. Mean Reward: 0.897. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 5770000. Time Elapsed: 77.262 s. Mean Reward: 0.899. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 5780000. Time Elapsed: 86.350 s. Mean Reward: 0.889. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 5790000. Time Elapsed: 105.495 s. Mean Reward: 0.883. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5800000. Time Elapsed: 114.447 s. Mean Reward: 0.885. Std of Reward: 0.104. Training.
[INFO] MoveAgent2D. Step: 5810000. Time Elapsed: 123.799 s. Mean Reward: 0.889. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 5820000. Time Elapsed: 132.791 s. Mean Reward: 0.895. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 5830000. Time Elapsed: 152.230 s. Mean Reward: 0.880. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 5840000. Time Elapsed: 161.316 s. Mean Reward: 0.901. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 5850000. Time Elapsed: 170.434 s. Mean Reward: 0.895. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 5860000. Time Elapsed: 179.621 s. Mean Reward: 0.895. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 5870000. Time Elapsed: 198.409 s. Mean Reward: 0.892. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5880000. Time Elapsed: 207.413 s. Mean Reward: 0.887. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 5890000. Time Elapsed: 217.237 s. Mean Reward: 0.892. Std of Reward: 0.084. Training.
[INFO] MoveAgent2D. Step: 5900000. Time Elapsed: 226.719 s. Mean Reward: 0.890. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 5910000. Time Elapsed: 246.021 s. Mean Reward: 0.897. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 5920000. Time Elapsed: 253.871 s. Mean Reward: 0.875. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 5930000. Time Elapsed: 262.878 s. Mean Reward: 0.896. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 5940000. Time Elapsed: 271.418 s. Mean Reward: 0.890. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 5950000. Time Elapsed: 290.931 s. Mean Reward: 0.892. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 5960000. Time Elapsed: 299.871 s. Mean Reward: 0.888. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 5970000. Time Elapsed: 308.828 s. Mean Reward: 0.889. Std of Reward: 0.094. Training.
[INFO] MoveAgent2D. Step: 5980000. Time Elapsed: 317.561 s. Mean Reward: 0.885. Std of Reward: 0.092. Training.
[INFO] MoveAgent2D. Step: 5990000. Time Elapsed: 337.710 s. Mean Reward: 0.897. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6000000. Time Elapsed: 346.635 s. Mean Reward: 0.891. Std of Reward: 0.080. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-5999922.onnx
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-6000013.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-6000013.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 20 - max_steps: 6300000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      6300000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 6000013.
[INFO] MoveAgent2D. Step: 6010000. Time Elapsed: 12.509 s. Mean Reward: 0.884. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 6020000. Time Elapsed: 21.217 s. Mean Reward: 0.892. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6030000. Time Elapsed: 30.418 s. Mean Reward: 0.893. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 6040000. Time Elapsed: 39.366 s. Mean Reward: 0.890. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 6050000. Time Elapsed: 58.923 s. Mean Reward: 0.900. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 6060000. Time Elapsed: 68.200 s. Mean Reward: 0.899. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6070000. Time Elapsed: 77.512 s. Mean Reward: 0.897. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 6080000. Time Elapsed: 86.789 s. Mean Reward: 0.891. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 6090000. Time Elapsed: 106.102 s. Mean Reward: 0.899. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6100000. Time Elapsed: 115.322 s. Mean Reward: 0.899. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6110000. Time Elapsed: 124.671 s. Mean Reward: 0.898. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6120000. Time Elapsed: 133.882 s. Mean Reward: 0.891. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 6130000. Time Elapsed: 153.223 s. Mean Reward: 0.901. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6140000. Time Elapsed: 162.262 s. Mean Reward: 0.894. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 6150000. Time Elapsed: 171.520 s. Mean Reward: 0.897. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6160000. Time Elapsed: 180.722 s. Mean Reward: 0.894. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6170000. Time Elapsed: 200.248 s. Mean Reward: 0.891. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6180000. Time Elapsed: 209.528 s. Mean Reward: 0.900. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 6190000. Time Elapsed: 218.686 s. Mean Reward: 0.894. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6200000. Time Elapsed: 228.075 s. Mean Reward: 0.897. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 6210000. Time Elapsed: 247.714 s. Mean Reward: 0.899. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 6220000. Time Elapsed: 256.828 s. Mean Reward: 0.900. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6230000. Time Elapsed: 266.261 s. Mean Reward: 0.902. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 6240000. Time Elapsed: 275.414 s. Mean Reward: 0.897. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6250000. Time Elapsed: 295.085 s. Mean Reward: 0.895. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 6260000. Time Elapsed: 304.136 s. Mean Reward: 0.895. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 6270000. Time Elapsed: 313.330 s. Mean Reward: 0.896. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6280000. Time Elapsed: 322.456 s. Mean Reward: 0.888. Std of Reward: 0.084. Training.
[INFO] MoveAgent2D. Step: 6290000. Time Elapsed: 342.357 s. Mean Reward: 0.900. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 6300000. Time Elapsed: 351.433 s. Mean Reward: 0.891. Std of Reward: 0.073. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-6300112.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-6300112.onnx to results\Player22\MoveAgent2D.onnx.
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source incrementalagent.sh Player22 10
Initial max_steps: 6300000
  ______   _________     _       _______   _________   _________  _______          _       _____  ____  _____  _____  ____  _____   ______
.' ____ \ |  _   _  |   / \     |_   __ \ |  _   _  | |  _   _  ||_   __ \        / \     |_   _||_   \|_   _||_   _||_   \|_   _|.' ___  |
| (___ \_||_/ | | \_|  / _ \      | |__) ||_/ | | \_| |_/ | | \_|  | |__) |      / _ \      | |    |   \ | |    | |    |   \ | | / .'   \_|
 _.____`.     | |     / ___ \     |  __ /     | |         | |      |  __ /      / ___ \     | |    | |\ \| |    | |    | |\ \| | | |   ____
| \____) |   _| |_  _/ /   \ \_  _| |  \ \_  _| |_       _| |_    _| |  \ \_  _/ /   \ \_  _| |_  _| |_\   |_  _| |_  _| |_\   |_\ `.___]  |
 \______.'  |_____||____| |____||____| |___||_____|     |_____|  |____| |___||____| |____||_____||_____|\____||_____||_____|\____|`._____.'

|---------------------------------------------|
|   Author: PhucThai                          |
|   Date create: June 12 2024                 |
|   Version: 1.0                              |
|   Folder run: results/Player22              |
|   Interation incremental: 10                 |
|   Max_step increase of flow: 9300000 steps |
|---------------------------------------------|


Start interval - max_steps: 6300000 - id=Player22


Interval 1 - max_steps: 6600000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      6600000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 6300112.
[INFO] MoveAgent2D. Step: 6310000. Time Elapsed: 9.018 s. Mean Reward: 0.900. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 6320000. Time Elapsed: 16.815 s. Mean Reward: 0.896. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6330000. Time Elapsed: 24.560 s. Mean Reward: 0.898. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6340000. Time Elapsed: 32.666 s. Mean Reward: 0.896. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6350000. Time Elapsed: 51.084 s. Mean Reward: 0.902. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6360000. Time Elapsed: 59.042 s. Mean Reward: 0.895. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6370000. Time Elapsed: 66.659 s. Mean Reward: 0.894. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6380000. Time Elapsed: 74.219 s. Mean Reward: 0.897. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 6390000. Time Elapsed: 91.806 s. Mean Reward: 0.896. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 6400000. Time Elapsed: 99.625 s. Mean Reward: 0.894. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6410000. Time Elapsed: 107.192 s. Mean Reward: 0.891. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 6420000. Time Elapsed: 114.859 s. Mean Reward: 0.891. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6430000. Time Elapsed: 132.547 s. Mean Reward: 0.895. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 6440000. Time Elapsed: 140.262 s. Mean Reward: 0.897. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6450000. Time Elapsed: 148.003 s. Mean Reward: 0.891. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 6460000. Time Elapsed: 155.761 s. Mean Reward: 0.903. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 6470000. Time Elapsed: 173.416 s. Mean Reward: 0.903. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 6480000. Time Elapsed: 181.245 s. Mean Reward: 0.903. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6490000. Time Elapsed: 188.987 s. Mean Reward: 0.895. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6500000. Time Elapsed: 196.650 s. Mean Reward: 0.904. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 6510000. Time Elapsed: 214.715 s. Mean Reward: 0.903. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6520000. Time Elapsed: 222.357 s. Mean Reward: 0.898. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 6530000. Time Elapsed: 230.218 s. Mean Reward: 0.902. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6540000. Time Elapsed: 237.971 s. Mean Reward: 0.894. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 6550000. Time Elapsed: 255.990 s. Mean Reward: 0.902. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 6560000. Time Elapsed: 263.560 s. Mean Reward: 0.897. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 6570000. Time Elapsed: 271.272 s. Mean Reward: 0.898. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 6580000. Time Elapsed: 278.902 s. Mean Reward: 0.901. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 6590000. Time Elapsed: 297.135 s. Mean Reward: 0.910. Std of Reward: 0.065. Training.
[INFO] MoveAgent2D. Step: 6600000. Time Elapsed: 304.994 s. Mean Reward: 0.904. Std of Reward: 0.071. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-6600036.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-6600036.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 2 - max_steps: 6900000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      6900000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 6600036.
[INFO] MoveAgent2D. Step: 6610000. Time Elapsed: 9.323 s. Mean Reward: 0.901. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 6620000. Time Elapsed: 16.900 s. Mean Reward: 0.897. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6630000. Time Elapsed: 24.507 s. Mean Reward: 0.892. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 6640000. Time Elapsed: 32.952 s. Mean Reward: 0.896. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 6650000. Time Elapsed: 50.859 s. Mean Reward: 0.904. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6660000. Time Elapsed: 59.319 s. Mean Reward: 0.904. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 6670000. Time Elapsed: 67.107 s. Mean Reward: 0.901. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6680000. Time Elapsed: 75.687 s. Mean Reward: 0.900. Std of Reward: 0.065. Training.
[INFO] MoveAgent2D. Step: 6690000. Time Elapsed: 93.585 s. Mean Reward: 0.890. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 6700000. Time Elapsed: 101.544 s. Mean Reward: 0.901. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6710000. Time Elapsed: 109.723 s. Mean Reward: 0.899. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6720000. Time Elapsed: 117.445 s. Mean Reward: 0.904. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6730000. Time Elapsed: 134.514 s. Mean Reward: 0.894. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 6740000. Time Elapsed: 141.955 s. Mean Reward: 0.896. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 6750000. Time Elapsed: 149.502 s. Mean Reward: 0.897. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6760000. Time Elapsed: 156.809 s. Mean Reward: 0.894. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6770000. Time Elapsed: 174.307 s. Mean Reward: 0.906. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 6780000. Time Elapsed: 181.769 s. Mean Reward: 0.897. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 6790000. Time Elapsed: 189.126 s. Mean Reward: 0.901. Std of Reward: 0.065. Training.
[INFO] MoveAgent2D. Step: 6800000. Time Elapsed: 196.923 s. Mean Reward: 0.904. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6810000. Time Elapsed: 214.244 s. Mean Reward: 0.897. Std of Reward: 0.092. Training.
[INFO] MoveAgent2D. Step: 6820000. Time Elapsed: 221.912 s. Mean Reward: 0.905. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 6830000. Time Elapsed: 229.266 s. Mean Reward: 0.901. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6840000. Time Elapsed: 236.808 s. Mean Reward: 0.895. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 6850000. Time Elapsed: 254.297 s. Mean Reward: 0.896. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 6860000. Time Elapsed: 261.760 s. Mean Reward: 0.895. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 6870000. Time Elapsed: 269.399 s. Mean Reward: 0.898. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6880000. Time Elapsed: 276.656 s. Mean Reward: 0.888. Std of Reward: 0.091. Training.
[INFO] MoveAgent2D. Step: 6890000. Time Elapsed: 294.014 s. Mean Reward: 0.891. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 6900000. Time Elapsed: 301.274 s. Mean Reward: 0.900. Std of Reward: 0.076. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-6900007.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-6900007.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 3 - max_steps: 7200000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      7200000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 6900007.
[INFO] MoveAgent2D. Step: 6910000. Time Elapsed: 9.388 s. Mean Reward: 0.909. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 6920000. Time Elapsed: 17.517 s. Mean Reward: 0.897. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 6930000. Time Elapsed: 25.163 s. Mean Reward: 0.898. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6940000. Time Elapsed: 33.287 s. Mean Reward: 0.903. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 6950000. Time Elapsed: 51.101 s. Mean Reward: 0.895. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 6960000. Time Elapsed: 58.391 s. Mean Reward: 0.883. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 6970000. Time Elapsed: 66.081 s. Mean Reward: 0.898. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 6980000. Time Elapsed: 73.271 s. Mean Reward: 0.883. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 6990000. Time Elapsed: 90.915 s. Mean Reward: 0.899. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 7000000. Time Elapsed: 98.361 s. Mean Reward: 0.904. Std of Reward: 0.073. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-6999985.onnx
[INFO] MoveAgent2D. Step: 7010000. Time Elapsed: 105.823 s. Mean Reward: 0.893. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 7020000. Time Elapsed: 113.159 s. Mean Reward: 0.898. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7030000. Time Elapsed: 130.486 s. Mean Reward: 0.895. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7040000. Time Elapsed: 137.922 s. Mean Reward: 0.895. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7050000. Time Elapsed: 145.451 s. Mean Reward: 0.897. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7060000. Time Elapsed: 152.860 s. Mean Reward: 0.893. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7070000. Time Elapsed: 170.409 s. Mean Reward: 0.902. Std of Reward: 0.064. Training.
[INFO] MoveAgent2D. Step: 7080000. Time Elapsed: 177.847 s. Mean Reward: 0.898. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 7090000. Time Elapsed: 185.260 s. Mean Reward: 0.895. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7100000. Time Elapsed: 192.667 s. Mean Reward: 0.899. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7110000. Time Elapsed: 210.482 s. Mean Reward: 0.903. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 7120000. Time Elapsed: 217.990 s. Mean Reward: 0.898. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 7130000. Time Elapsed: 225.415 s. Mean Reward: 0.900. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 7140000. Time Elapsed: 232.941 s. Mean Reward: 0.905. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 7150000. Time Elapsed: 250.649 s. Mean Reward: 0.900. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 7160000. Time Elapsed: 257.915 s. Mean Reward: 0.896. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 7170000. Time Elapsed: 265.344 s. Mean Reward: 0.902. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 7180000. Time Elapsed: 272.927 s. Mean Reward: 0.900. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7190000. Time Elapsed: 290.473 s. Mean Reward: 0.895. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7200000. Time Elapsed: 297.876 s. Mean Reward: 0.904. Std of Reward: 0.071. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-7200052.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-7200052.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 4 - max_steps: 7500000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      7500000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 7200052.
[INFO] MoveAgent2D. Step: 7210000. Time Elapsed: 9.168 s. Mean Reward: 0.899. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 7220000. Time Elapsed: 16.672 s. Mean Reward: 0.891. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7230000. Time Elapsed: 24.370 s. Mean Reward: 0.895. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7240000. Time Elapsed: 32.149 s. Mean Reward: 0.900. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 7250000. Time Elapsed: 49.871 s. Mean Reward: 0.901. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 7260000. Time Elapsed: 57.510 s. Mean Reward: 0.895. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7270000. Time Elapsed: 65.183 s. Mean Reward: 0.903. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 7280000. Time Elapsed: 72.832 s. Mean Reward: 0.897. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7290000. Time Elapsed: 90.607 s. Mean Reward: 0.897. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 7300000. Time Elapsed: 98.333 s. Mean Reward: 0.901. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 7310000. Time Elapsed: 106.103 s. Mean Reward: 0.904. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7320000. Time Elapsed: 113.790 s. Mean Reward: 0.894. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 7330000. Time Elapsed: 131.339 s. Mean Reward: 0.893. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 7340000. Time Elapsed: 139.007 s. Mean Reward: 0.894. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 7350000. Time Elapsed: 146.516 s. Mean Reward: 0.899. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7360000. Time Elapsed: 154.168 s. Mean Reward: 0.900. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 7370000. Time Elapsed: 171.877 s. Mean Reward: 0.904. Std of Reward: 0.065. Training.
[INFO] MoveAgent2D. Step: 7380000. Time Elapsed: 179.452 s. Mean Reward: 0.895. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7390000. Time Elapsed: 187.546 s. Mean Reward: 0.901. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 7400000. Time Elapsed: 195.199 s. Mean Reward: 0.898. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7410000. Time Elapsed: 212.876 s. Mean Reward: 0.899. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 7420000. Time Elapsed: 220.332 s. Mean Reward: 0.898. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 7430000. Time Elapsed: 228.104 s. Mean Reward: 0.899. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 7440000. Time Elapsed: 235.821 s. Mean Reward: 0.903. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 7450000. Time Elapsed: 253.644 s. Mean Reward: 0.904. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 7460000. Time Elapsed: 261.105 s. Mean Reward: 0.873. Std of Reward: 0.221. Training.
[INFO] MoveAgent2D. Step: 7470000. Time Elapsed: 268.615 s. Mean Reward: 0.874. Std of Reward: 0.233. Training.
[INFO] MoveAgent2D. Step: 7480000. Time Elapsed: 276.176 s. Mean Reward: 0.845. Std of Reward: 0.321. Training.
[INFO] MoveAgent2D. Step: 7490000. Time Elapsed: 293.734 s. Mean Reward: 0.861. Std of Reward: 0.309. Training.
[INFO] MoveAgent2D. Step: 7500000. Time Elapsed: 301.191 s. Mean Reward: 0.870. Std of Reward: 0.284. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-7500003.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-7500003.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 5 - max_steps: 7800000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      7800000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 7500003.
[INFO] MoveAgent2D. Step: 7510000. Time Elapsed: 9.037 s. Mean Reward: 0.897. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 7520000. Time Elapsed: 16.646 s. Mean Reward: 0.892. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 7530000. Time Elapsed: 24.146 s. Mean Reward: 0.893. Std of Reward: 0.091. Training.
[INFO] MoveAgent2D. Step: 7540000. Time Elapsed: 31.762 s. Mean Reward: 0.891. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 7550000. Time Elapsed: 49.593 s. Mean Reward: 0.764. Std of Reward: 0.604. Training.
[INFO] MoveAgent2D. Step: 7560000. Time Elapsed: 56.580 s. Mean Reward: 0.537. Std of Reward: 0.932. Training.
[INFO] MoveAgent2D. Step: 7570000. Time Elapsed: 63.918 s. Mean Reward: 0.612. Std of Reward: 0.846. Training.
[INFO] MoveAgent2D. Step: 7580000. Time Elapsed: 71.125 s. Mean Reward: 0.634. Std of Reward: 0.797. Training.
[INFO] MoveAgent2D. Step: 7590000. Time Elapsed: 88.928 s. Mean Reward: 0.758. Std of Reward: 0.588. Training.
[INFO] MoveAgent2D. Step: 7600000. Time Elapsed: 96.185 s. Mean Reward: 0.837. Std of Reward: 0.338. Training.
[INFO] MoveAgent2D. Step: 7610000. Time Elapsed: 104.274 s. Mean Reward: 0.820. Std of Reward: 0.442. Training.
[INFO] MoveAgent2D. Step: 7620000. Time Elapsed: 112.076 s. Mean Reward: 0.768. Std of Reward: 0.558. Training.
[INFO] MoveAgent2D. Step: 7630000. Time Elapsed: 130.189 s. Mean Reward: 0.872. Std of Reward: 0.227. Training.
[INFO] MoveAgent2D. Step: 7640000. Time Elapsed: 138.219 s. Mean Reward: 0.838. Std of Reward: 0.331. Training.
[INFO] MoveAgent2D. Step: 7650000. Time Elapsed: 146.036 s. Mean Reward: 0.843. Std of Reward: 0.371. Training.
[INFO] MoveAgent2D. Step: 7660000. Time Elapsed: 153.831 s. Mean Reward: 0.848. Std of Reward: 0.167. Training.
[INFO] MoveAgent2D. Step: 7670000. Time Elapsed: 171.218 s. Mean Reward: 0.888. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 7680000. Time Elapsed: 178.589 s. Mean Reward: 0.878. Std of Reward: 0.220. Training.
[INFO] MoveAgent2D. Step: 7690000. Time Elapsed: 185.928 s. Mean Reward: 0.883. Std of Reward: 0.215. Training.
[INFO] MoveAgent2D. Step: 7700000. Time Elapsed: 193.356 s. Mean Reward: 0.894. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 7710000. Time Elapsed: 210.898 s. Mean Reward: 0.887. Std of Reward: 0.101. Training.
[INFO] MoveAgent2D. Step: 7720000. Time Elapsed: 218.342 s. Mean Reward: 0.888. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 7730000. Time Elapsed: 225.776 s. Mean Reward: 0.895. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 7740000. Time Elapsed: 233.224 s. Mean Reward: 0.896. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 7750000. Time Elapsed: 250.746 s. Mean Reward: 0.887. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 7760000. Time Elapsed: 258.159 s. Mean Reward: 0.903. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 7770000. Time Elapsed: 265.568 s. Mean Reward: 0.894. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 7780000. Time Elapsed: 273.164 s. Mean Reward: 0.891. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 7790000. Time Elapsed: 290.529 s. Mean Reward: 0.899. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 7800000. Time Elapsed: 297.944 s. Mean Reward: 0.895. Std of Reward: 0.077. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-7800015.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-7800015.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 6 - max_steps: 8100000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      8100000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 7800015.
[INFO] MoveAgent2D. Step: 7810000. Time Elapsed: 9.151 s. Mean Reward: 0.905. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 7820000. Time Elapsed: 16.763 s. Mean Reward: 0.896. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 7830000. Time Elapsed: 24.426 s. Mean Reward: 0.901. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 7840000. Time Elapsed: 32.023 s. Mean Reward: 0.891. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 7850000. Time Elapsed: 50.171 s. Mean Reward: 0.904. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7860000. Time Elapsed: 58.129 s. Mean Reward: 0.887. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 7870000. Time Elapsed: 65.925 s. Mean Reward: 0.904. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 7880000. Time Elapsed: 73.522 s. Mean Reward: 0.893. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 7890000. Time Elapsed: 91.733 s. Mean Reward: 0.889. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 7900000. Time Elapsed: 99.196 s. Mean Reward: 0.883. Std of Reward: 0.107. Training.
[INFO] MoveAgent2D. Step: 7910000. Time Elapsed: 106.988 s. Mean Reward: 0.896. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 7920000. Time Elapsed: 114.785 s. Mean Reward: 0.898. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 7930000. Time Elapsed: 132.841 s. Mean Reward: 0.898. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 7940000. Time Elapsed: 140.494 s. Mean Reward: 0.892. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 7950000. Time Elapsed: 148.254 s. Mean Reward: 0.897. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 7960000. Time Elapsed: 155.820 s. Mean Reward: 0.892. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 7970000. Time Elapsed: 173.885 s. Mean Reward: 0.898. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 7980000. Time Elapsed: 181.543 s. Mean Reward: 0.895. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 7990000. Time Elapsed: 189.375 s. Mean Reward: 0.892. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8000000. Time Elapsed: 197.128 s. Mean Reward: 0.894. Std of Reward: 0.074. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-7999989.onnx
[INFO] MoveAgent2D. Step: 8010000. Time Elapsed: 215.104 s. Mean Reward: 0.895. Std of Reward: 0.081. Training.
[INFO] MoveAgent2D. Step: 8020000. Time Elapsed: 222.817 s. Mean Reward: 0.898. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8030000. Time Elapsed: 230.554 s. Mean Reward: 0.896. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8040000. Time Elapsed: 238.220 s. Mean Reward: 0.894. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8050000. Time Elapsed: 256.550 s. Mean Reward: 0.903. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8060000. Time Elapsed: 264.244 s. Mean Reward: 0.899. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 8070000. Time Elapsed: 272.089 s. Mean Reward: 0.904. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 8080000. Time Elapsed: 280.009 s. Mean Reward: 0.904. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 8090000. Time Elapsed: 297.978 s. Mean Reward: 0.896. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8100000. Time Elapsed: 305.970 s. Mean Reward: 0.899. Std of Reward: 0.069. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-8100002.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-8100002.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 7 - max_steps: 8400000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      8400000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 8100002.
[INFO] MoveAgent2D. Step: 8110000. Time Elapsed: 9.460 s. Mean Reward: 0.907. Std of Reward: 0.059. Training.
[INFO] MoveAgent2D. Step: 8120000. Time Elapsed: 17.472 s. Mean Reward: 0.895. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8130000. Time Elapsed: 25.270 s. Mean Reward: 0.898. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 8140000. Time Elapsed: 32.933 s. Mean Reward: 0.899. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 8150000. Time Elapsed: 50.141 s. Mean Reward: 0.896. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8160000. Time Elapsed: 57.642 s. Mean Reward: 0.887. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 8170000. Time Elapsed: 65.485 s. Mean Reward: 0.902. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 8180000. Time Elapsed: 73.080 s. Mean Reward: 0.896. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8190000. Time Elapsed: 90.948 s. Mean Reward: 0.893. Std of Reward: 0.088. Training.
[INFO] MoveAgent2D. Step: 8200000. Time Elapsed: 98.561 s. Mean Reward: 0.896. Std of Reward: 0.095. Training.
[INFO] MoveAgent2D. Step: 8210000. Time Elapsed: 106.092 s. Mean Reward: 0.894. Std of Reward: 0.096. Training.
[INFO] MoveAgent2D. Step: 8220000. Time Elapsed: 113.695 s. Mean Reward: 0.886. Std of Reward: 0.105. Training.
[INFO] MoveAgent2D. Step: 8230000. Time Elapsed: 131.376 s. Mean Reward: 0.896. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 8240000. Time Elapsed: 138.904 s. Mean Reward: 0.895. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8250000. Time Elapsed: 146.674 s. Mean Reward: 0.892. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 8260000. Time Elapsed: 154.155 s. Mean Reward: 0.896. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8270000. Time Elapsed: 172.044 s. Mean Reward: 0.902. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8280000. Time Elapsed: 179.514 s. Mean Reward: 0.894. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8290000. Time Elapsed: 187.279 s. Mean Reward: 0.904. Std of Reward: 0.064. Training.
[INFO] MoveAgent2D. Step: 8300000. Time Elapsed: 194.887 s. Mean Reward: 0.893. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 8310000. Time Elapsed: 212.371 s. Mean Reward: 0.897. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 8320000. Time Elapsed: 219.921 s. Mean Reward: 0.895. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8330000. Time Elapsed: 227.641 s. Mean Reward: 0.899. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 8340000. Time Elapsed: 235.156 s. Mean Reward: 0.893. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8350000. Time Elapsed: 252.878 s. Mean Reward: 0.896. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 8360000. Time Elapsed: 260.499 s. Mean Reward: 0.901. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8370000. Time Elapsed: 268.124 s. Mean Reward: 0.897. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 8380000. Time Elapsed: 275.666 s. Mean Reward: 0.900. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 8390000. Time Elapsed: 293.398 s. Mean Reward: 0.899. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 8400000. Time Elapsed: 301.097 s. Mean Reward: 0.905. Std of Reward: 0.070. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-8400074.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-8400074.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 8 - max_steps: 8700000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      8700000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 8400074.
[INFO] MoveAgent2D. Step: 8410000. Time Elapsed: 9.018 s. Mean Reward: 0.900. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 8420000. Time Elapsed: 16.883 s. Mean Reward: 0.899. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8430000. Time Elapsed: 24.450 s. Mean Reward: 0.897. Std of Reward: 0.086. Training.
[INFO] MoveAgent2D. Step: 8440000. Time Elapsed: 32.088 s. Mean Reward: 0.897. Std of Reward: 0.065. Training.
[INFO] MoveAgent2D. Step: 8450000. Time Elapsed: 49.934 s. Mean Reward: 0.899. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8460000. Time Elapsed: 57.604 s. Mean Reward: 0.898. Std of Reward: 0.065. Training.
[INFO] MoveAgent2D. Step: 8470000. Time Elapsed: 65.262 s. Mean Reward: 0.902. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8480000. Time Elapsed: 72.799 s. Mean Reward: 0.895. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 8490000. Time Elapsed: 90.612 s. Mean Reward: 0.901. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 8500000. Time Elapsed: 98.405 s. Mean Reward: 0.901. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 8510000. Time Elapsed: 105.965 s. Mean Reward: 0.897. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 8520000. Time Elapsed: 113.479 s. Mean Reward: 0.888. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8530000. Time Elapsed: 131.274 s. Mean Reward: 0.894. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 8540000. Time Elapsed: 138.634 s. Mean Reward: 0.879. Std of Reward: 0.126. Training.
[INFO] MoveAgent2D. Step: 8550000. Time Elapsed: 146.344 s. Mean Reward: 0.901. Std of Reward: 0.078. Training.
[INFO] MoveAgent2D. Step: 8560000. Time Elapsed: 153.902 s. Mean Reward: 0.876. Std of Reward: 0.213. Training.
[INFO] MoveAgent2D. Step: 8570000. Time Elapsed: 171.619 s. Mean Reward: 0.884. Std of Reward: 0.204. Training.
[INFO] MoveAgent2D. Step: 8580000. Time Elapsed: 179.163 s. Mean Reward: 0.896. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 8590000. Time Elapsed: 186.801 s. Mean Reward: 0.903. Std of Reward: 0.072. Training.
[INFO] MoveAgent2D. Step: 8600000. Time Elapsed: 194.537 s. Mean Reward: 0.902. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 8610000. Time Elapsed: 212.464 s. Mean Reward: 0.905. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 8620000. Time Elapsed: 220.028 s. Mean Reward: 0.898. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 8630000. Time Elapsed: 227.608 s. Mean Reward: 0.894. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8640000. Time Elapsed: 235.408 s. Mean Reward: 0.904. Std of Reward: 0.064. Training.
[INFO] MoveAgent2D. Step: 8650000. Time Elapsed: 253.170 s. Mean Reward: 0.903. Std of Reward: 0.066. Training.
[INFO] MoveAgent2D. Step: 8660000. Time Elapsed: 260.829 s. Mean Reward: 0.900. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8670000. Time Elapsed: 268.352 s. Mean Reward: 0.892. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 8680000. Time Elapsed: 275.891 s. Mean Reward: 0.897. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 8690000. Time Elapsed: 293.703 s. Mean Reward: 0.902. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 8700000. Time Elapsed: 301.245 s. Mean Reward: 0.898. Std of Reward: 0.071. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-8700057.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-8700057.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 9 - max_steps: 9000000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      9000000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 8700057.
[INFO] MoveAgent2D. Step: 8710000. Time Elapsed: 8.999 s. Mean Reward: 0.899. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8720000. Time Elapsed: 16.588 s. Mean Reward: 0.896. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 8730000. Time Elapsed: 24.224 s. Mean Reward: 0.900. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8740000. Time Elapsed: 31.929 s. Mean Reward: 0.900. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 8750000. Time Elapsed: 49.704 s. Mean Reward: 0.893. Std of Reward: 0.077. Training.
[INFO] MoveAgent2D. Step: 8760000. Time Elapsed: 57.197 s. Mean Reward: 0.888. Std of Reward: 0.102. Training.
[INFO] MoveAgent2D. Step: 8770000. Time Elapsed: 64.678 s. Mean Reward: 0.883. Std of Reward: 0.093. Training.
[INFO] MoveAgent2D. Step: 8780000. Time Elapsed: 72.135 s. Mean Reward: 0.849. Std of Reward: 0.358. Training.
[INFO] MoveAgent2D. Step: 8790000. Time Elapsed: 89.744 s. Mean Reward: 0.894. Std of Reward: 0.090. Training.
[INFO] MoveAgent2D. Step: 8800000. Time Elapsed: 97.365 s. Mean Reward: 0.892. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 8810000. Time Elapsed: 104.977 s. Mean Reward: 0.896. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8820000. Time Elapsed: 112.634 s. Mean Reward: 0.894. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8830000. Time Elapsed: 130.354 s. Mean Reward: 0.900. Std of Reward: 0.068. Training.
[INFO] MoveAgent2D. Step: 8840000. Time Elapsed: 138.092 s. Mean Reward: 0.892. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 8850000. Time Elapsed: 145.604 s. Mean Reward: 0.890. Std of Reward: 0.080. Training.
[INFO] MoveAgent2D. Step: 8860000. Time Elapsed: 153.090 s. Mean Reward: 0.894. Std of Reward: 0.073. Training.
[INFO] MoveAgent2D. Step: 8870000. Time Elapsed: 171.020 s. Mean Reward: 0.894. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8880000. Time Elapsed: 178.615 s. Mean Reward: 0.897. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8890000. Time Elapsed: 186.204 s. Mean Reward: 0.903. Std of Reward: 0.071. Training.
[INFO] MoveAgent2D. Step: 8900000. Time Elapsed: 193.823 s. Mean Reward: 0.894. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 8910000. Time Elapsed: 211.566 s. Mean Reward: 0.897. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 8920000. Time Elapsed: 219.050 s. Mean Reward: 0.895. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 8930000. Time Elapsed: 226.690 s. Mean Reward: 0.900. Std of Reward: 0.076. Training.
[INFO] MoveAgent2D. Step: 8940000. Time Elapsed: 234.233 s. Mean Reward: 0.902. Std of Reward: 0.069. Training.
[INFO] MoveAgent2D. Step: 8950000. Time Elapsed: 251.865 s. Mean Reward: 0.901. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 8960000. Time Elapsed: 259.422 s. Mean Reward: 0.901. Std of Reward: 0.070. Training.
[INFO] MoveAgent2D. Step: 8970000. Time Elapsed: 267.335 s. Mean Reward: 0.902. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 8980000. Time Elapsed: 274.634 s. Mean Reward: 0.889. Std of Reward: 0.075. Training.
[INFO] MoveAgent2D. Step: 8990000. Time Elapsed: 292.148 s. Mean Reward: 0.899. Std of Reward: 0.067. Training.
[INFO] MoveAgent2D. Step: 9000000. Time Elapsed: 299.373 s. Mean Reward: 0.725. Std of Reward: 0.678. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-8999883.onnx
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-9000011.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-9000011.onnx to results\Player22\MoveAgent2D.onnx.

Sleep for a while!

Interval 10 - max_steps: 9300000 - id=Player22

D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
D:\UnityProject\MLAgents_EatFruit\venv\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: MoveAgent2D?team=0


        Unity Technologies

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.2.2+cu121
[INFO] Hyperparameters for behavior name MoveAgent2D:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   256
          buffer_size:  40960
          learning_rate:        0.0005
          beta: 0.04
          epsilon:      0.23
          lambd:        0.9875
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       7
        checkpoint_interval:    1000000
        max_steps:      9300000
        time_horizon:   128
        summary_freq:   10000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] Resuming from results\Player22\MoveAgent2D.
[INFO] Resuming training from step 9000011.
[INFO] MoveAgent2D. Step: 9010000. Time Elapsed: 9.125 s. Mean Reward: 0.836. Std of Reward: 0.424. Training.
[INFO] MoveAgent2D. Step: 9020000. Time Elapsed: 16.168 s. Mean Reward: 0.647. Std of Reward: 0.818. Training.
[INFO] MoveAgent2D. Step: 9030000. Time Elapsed: 23.284 s. Mean Reward: 0.655. Std of Reward: 0.787. Training.
[INFO] MoveAgent2D. Step: 9040000. Time Elapsed: 30.716 s. Mean Reward: 0.697. Std of Reward: 0.718. Training.
[INFO] MoveAgent2D. Step: 9050000. Time Elapsed: 47.921 s. Mean Reward: 0.730. Std of Reward: 0.665. Training.
[INFO] MoveAgent2D. Step: 9060000. Time Elapsed: 55.328 s. Mean Reward: 0.869. Std of Reward: 0.228. Training.
[INFO] MoveAgent2D. Step: 9070000. Time Elapsed: 62.740 s. Mean Reward: 0.838. Std of Reward: 0.378. Training.
[INFO] MoveAgent2D. Step: 9080000. Time Elapsed: 70.150 s. Mean Reward: 0.795. Std of Reward: 0.465. Training.
[INFO] MoveAgent2D. Step: 9090000. Time Elapsed: 87.529 s. Mean Reward: 0.851. Std of Reward: 0.305. Training.
[INFO] MoveAgent2D. Step: 9100000. Time Elapsed: 95.036 s. Mean Reward: 0.866. Std of Reward: 0.287. Training.
[INFO] MoveAgent2D. Step: 9110000. Time Elapsed: 102.563 s. Mean Reward: 0.853. Std of Reward: 0.354. Training.
[INFO] MoveAgent2D. Step: 9120000. Time Elapsed: 110.085 s. Mean Reward: 0.865. Std of Reward: 0.237. Training.
[INFO] MoveAgent2D. Step: 9130000. Time Elapsed: 127.896 s. Mean Reward: 0.880. Std of Reward: 0.208. Training.
[INFO] MoveAgent2D. Step: 9140000. Time Elapsed: 135.470 s. Mean Reward: 0.891. Std of Reward: 0.089. Training.
[INFO] MoveAgent2D. Step: 9150000. Time Elapsed: 143.050 s. Mean Reward: 0.900. Std of Reward: 0.085. Training.
[INFO] MoveAgent2D. Step: 9160000. Time Elapsed: 150.728 s. Mean Reward: 0.890. Std of Reward: 0.082. Training.
[INFO] MoveAgent2D. Step: 9170000. Time Elapsed: 168.142 s. Mean Reward: 0.875. Std of Reward: 0.279. Training.
[INFO] MoveAgent2D. Step: 9180000. Time Elapsed: 175.622 s. Mean Reward: 0.890. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 9190000. Time Elapsed: 183.231 s. Mean Reward: 0.893. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 9200000. Time Elapsed: 190.976 s. Mean Reward: 0.899. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 9210000. Time Elapsed: 208.628 s. Mean Reward: 0.893. Std of Reward: 0.093. Training.
[INFO] MoveAgent2D. Step: 9220000. Time Elapsed: 216.064 s. Mean Reward: 0.860. Std of Reward: 0.236. Training.
[INFO] MoveAgent2D. Step: 9230000. Time Elapsed: 223.704 s. Mean Reward: 0.860. Std of Reward: 0.239. Training.
[INFO] MoveAgent2D. Step: 9240000. Time Elapsed: 230.974 s. Mean Reward: 0.864. Std of Reward: 0.139. Training.
[INFO] MoveAgent2D. Step: 9250000. Time Elapsed: 248.390 s. Mean Reward: 0.830. Std of Reward: 0.332. Training.
[INFO] MoveAgent2D. Step: 9260000. Time Elapsed: 256.037 s. Mean Reward: 0.894. Std of Reward: 0.074. Training.
[INFO] MoveAgent2D. Step: 9270000. Time Elapsed: 263.612 s. Mean Reward: 0.886. Std of Reward: 0.087. Training.
[INFO] MoveAgent2D. Step: 9280000. Time Elapsed: 271.086 s. Mean Reward: 0.892. Std of Reward: 0.083. Training.
[INFO] MoveAgent2D. Step: 9290000. Time Elapsed: 288.833 s. Mean Reward: 0.894. Std of Reward: 0.079. Training.
[INFO] MoveAgent2D. Step: 9300000. Time Elapsed: 296.355 s. Mean Reward: 0.891. Std of Reward: 0.081. Training.
[INFO] Exported results\Player22\MoveAgent2D\MoveAgent2D-9300046.onnx
[INFO] Copied results\Player22\MoveAgent2D\MoveAgent2D-9300046.onnx to results\Player22\MoveAgent2D.onnx.
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ vim getresult.sh
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ vim runmlagent.sh
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ vim getresult.sh
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ ll -rt
total 530
-rw-r--r-- 1 nhancao 197121   1917 May 30 12:58  MLAgents_EatFruit.sln
drwxr-xr-x 1 nhancao 197121      0 May 30 13:33  venv/
drwxr-xr-x 1 nhancao 197121      0 Jun  3 14:01  obj/
drwxr-xr-x 1 nhancao 197121      0 Jun  6 10:15  MoveAgent2DDemo/
-rw-r--r-- 1 nhancao 197121  43293 Jun  7 23:20 'Cumulative Reward Extend.PNG'
-rw-r--r-- 1 nhancao 197121  49162 Jun  7 23:21  Env.PNG
drwxr-xr-x 1 nhancao 197121      0 Jun 11 14:51  Packages/
-rw-r--r-- 1 nhancao 197121 140309 Jun 11 21:20  logfile.txt
-rwxr-xr-x 1 nhancao 197121    205 Jun 11 21:22  getlog.sh*
-rw-r--r-- 1 nhancao 197121  11854 Jun 11 21:22  filledlog.txt
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:37  UserSettings/
drwxr-xr-x 1 nhancao 197121      0 Jun 12 18:38  Build/
-rwxr-xr-x 1 nhancao 197121   3882 Jun 13 09:28  runmlagent.sh*
drwxr-xr-x 1 nhancao 197121      0 Jun 13 09:30  Logs/
-rw-r--r-- 1 nhancao 197121  76025 Jun 13 09:30  Assembly-CSharp-firstpass.csproj
-rw-r--r-- 1 nhancao 197121  77392 Jun 13 09:30  Assembly-CSharp.csproj
-rw-r--r-- 1 nhancao 197121  80598 Jun 13 09:30  Assembly-CSharp-Editor.csproj
drwxr-xr-x 1 nhancao 197121      0 Jun 13 09:30  Library/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 09:30  Temp/
-rwxr-xr-x 1 nhancao 197121   2630 Jun 13 11:55  incrementalagent.sh*
drwxr-xr-x 1 nhancao 197121      0 Jun 13 12:18  results/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 12:23  Assets/
drwxr-xr-x 1 nhancao 197121      0 Jun 13 12:23  ProjectSettings/
-rwxr-xr-x 1 nhancao 197121     41 Jun 13 12:30  getresult.sh*
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source  get
getlog.sh     getresult.sh
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source  get
getlog.sh     getresult.sh
(venv)
nhancao@nhancao-windows MINGW64 /d/UnityProject/MLAgents_EatFruit (main)
$ source  getresult.sh
TensorFlow installation not found - running with reduced feature set.
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)
